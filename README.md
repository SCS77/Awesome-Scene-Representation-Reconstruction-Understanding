<h1 align="center">
  <b>Scene-Reconstruction-Understanding </b>
</h1>

ğŸ“¦ Curated collection of papers, datasets, codebases, and resources in the field of scene reconstruction and understanding using NeRF and 3DGS.

## ğŸ˜„ğŸ˜„ <span style="color:red;">Under Construction</span>  ğŸ˜„ğŸ˜„


## [ğŸ”¥ Scene Reconstruction](#scene-reconstruction)  

### â€¢ [NeRF(Neural Radiance Fields)](#nerf) 
### â€¢ [3DGS(3D Gaussian Splatting)](#3dgs)  


## [ğŸš€ Scene Understanding](#scene-understanding)  


---


## Scene Reconstruction

### NeRF & 3DGS

| Code | Paper | Title | Venue | Year |
|------|-------|-------|-------|------|
| [GitHub](https://github.com/bmild/nerf) | [Paper](https://arxiv.org/abs/2003.08934) | **NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis ğŸ”¥ğŸ”¥** | ECCV | 2020 |
| [GitHub](https://github.com/nerfstudio-project/nerfstudio) | ------ | **nerfstudio ğŸ”¥ğŸ”¥** | ---- | 2022 |
| [GitHub](https://github.com/Xharlie/pointnerf) | [Paper](https://arxiv.org/pdf/2201.08845) | **Point-NeRF: Point-based Neural Radiance Fields ğŸ”¥ğŸ”¥** | CVPR | 2020 |
| [GitHub](https://github.com/NVlabs/instant-ngp) | [Paper](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf) | **Instant Neural Graphics Primitives with a Multiresolution Hash Encoding ğŸ”¥ğŸ”¥** | TOG | 2022 |
|------|-------|-------|-------|------|
| [GitHub](https://github.com/graphdeco-inria/gaussian-splatting) | [Paper](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/3d_gaussian_splatting_high.pdf) | **3D Gaussian Splatting for Real-Time Radiance Field Rendering ğŸ”¥ğŸ”¥** | TOG | 2023 |
| [GitHub](https://github.com/zju3dv/EasyVolcap) | [Paper](https://drive.google.com/file/d/1J9Hl7KPMOkBmrt6UTFf_u9sQcjbblILA/view) | **Representing Long Volumetric Video with Temporal Gaussian Hierarchy ğŸ”¥ğŸ”¥** | TOG | 2024 |
| [GitHub](https://github.com/hbb1/2d-gaussian-splatting) | [Paper](https://arxiv.org/abs/2403.17888) | **2D Gaussian Splatting for Geometrically Accurate Radiance Fields ğŸ”¥ğŸ”¥** | SIGGRAPH | 2024 |
| [GitHub](https://github.com/ingra14m/Deformable-3D-Gaussians) | [Paper](https://arxiv.org/abs/2309.13101) | **Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction ğŸ”¥ğŸ”¥** | CVPR | 2024 |
| [GitHub](https://github.com/hustvl/4DGaussians) | [Paper](https://arxiv.org/pdf/2310.08528v2) | **4D Gaussian Splatting for Real-Time Dynamic Scene Rendering ğŸ”¥ğŸ”¥** | CVPR | 2024 |




## Scene understanding

#### Geometry-based and learning-based

| Code | Paper | Title | Venue | Year |
|------|-------|-------|-------|------|
| [GitHub](https://github.com/charlesq34/pointnet.git) | [Paper](https://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf) | **PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation ğŸ”¥** | CVPR | 2017 |
| [GitHub](https://github.com/charlesq34/pointnet2.git) | [Paper](https://dl.acm.org/doi/abs/10.5555/3295222.3295263) | **PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space ğŸ”¥** | NIPS | 2017 |
| [GitHub](https://github.com/qianguih/voxelnet) | [paper](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_VoxelNet_End-to-End_Learning_CVPR_2018_paper.pdf) | **VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection** | CVPR | 2018 |
| [GitHub](https://github.com/yangyanli/PointCNN) | [Paper](https://proceedings.neurips.cc/paper_files/paper/2018/file/f5f8590cd58a54e94377e6ae2eded4d9-Paper.pdf) | **PointCNN: Convolution On X-Transformed Points ğŸ”¥** | NIPS | 2018 |
| [GitHub](https://github.com/WangYueFt/dgcnn.git) | [Paper](https://dl.acm.org/doi/abs/10.1145/3326362) | **Dynamic Graph CNN for Learning on Point Clouds ğŸ”¥** | TOG | 2019 |
| [GitHub](https://github.com/QingyongHu/RandLA-Net.git) | [Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_RandLA-Net_Efficient_Semantic_Segmentation_of_Large-Scale_Point_Clouds_CVPR_2020_paper.pdf) | **RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds ğŸ”¥** | CVPR | 2020 |
| [GitHub](https://github.com/POSTECH-CVLab/point-transformer.git) | [Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Point_Transformer_ICCV_2021_paper.pdf) | **Point Transformer ğŸ”¥** | ICCV | 2021 |
| [GitHub](https://github.com/Pointcept/PointTransformerV3.git) | [Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Point_Transformer_V3_Simpler_Faster_Stronger_CVPR_2024_paper.pdf) | **Point Transformer V3: Simpler, Faster, Stronger ğŸ”¥ğŸ”¥** | CVPR | 2024 |
| [GitHub](https://github.com/Pointcept/Pointcept) | [Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Peng_OA-CNNs_Omni-Adaptive_Sparse_CNNs_for_3D_Semantic_Segmentation_CVPR_2024_paper.pdf) | **OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation ğŸ”¥** | CVPR | 2024 |
|------|-------|-------|-------|------|
| [GitHub](https://github.com/google-research/scenic) | [Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_TransMVSNet_Global_Context-Aware_Multi-View_Stereo_Network_With_Transformers_CVPR_2022_paper.pdf) | **Multiview Transformers for Video Recognition ğŸ”¥ğŸ”¥** | CVPR | 2022 |  
| [GitHub](https://github.com/bradyz/cross_view_transformers) | [Paper](https://arxiv.org/abs/2205.02833) | **Cross-view Transformers for real-time Map-view Semantic Segmentation** | CVPR | 2022 |  
| [GitHub](https://github.com/megvii-research/TransMVSNet) | [Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_TransMVSNet_Global_Context-Aware_Multi-View_Stereo_Network_With_Transformers_CVPR_2022_paper.pdf) | **TransMVSNet: Global Context-aware Multi-view Stereo Network with Transformers ğŸ”¥** | CVPR | 2022 |
| [GitHub](https://github.com/naver/dust3r) | [Paper](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_DUSt3R_Geometric_3D_Vision_Made_Easy_CVPR_2024_paper.html) | **DUSt3R: Geometric 3D Vision Made Easy ğŸ”¥ğŸ”¥** | CVPR | 2024 |
| [GitHub](https://github.com/naver/must3r) | [Paper](https://openaccess.thecvf.com/content/CVPR2025/html/Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction_CVPR_2025_paper.html) | **MUSt3R: Multi-view Network for Stereo 3D Reconstruction ğŸ”¥** | CVPR | 2025 |
| [GitHub](https://github.com/naver/pow3r) | [Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Jang_Pow3R_Empowering_Unconstrained_3D_Reconstruction_with_Camera_and_Scene_Priors_CVPR_2025_paper.pdf#:~:text=In%20this%20paper%2C%20we%20introduce%20Pow3R%2C%20a%20new,sparse%20or%20dense%20depth%2C%20or%20relative%20camera%20poses.) | **Pow3R: Empowering Unconstrained 3D  Reconstruction with Camera and Scene Priors** | CVPR | 2025 |
| [GitHub](https://github.com/facebookresearch/mvdust3r) | [Paper](https://arxiv.org/abs/2412.06974) | **MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds** | CVPR | 2025 |
| [GitHub](https://github.com/HKUST-SAIL/sail-recon) | [Paper](https://arxiv.org/abs/2508.17972) | **SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization** | Arxiv | 2025 |
| [GitHub](https://github.com/CUT3R/CUT3R) | [Paper](https://arxiv.org/pdf/2501.12387) | **Continuous 3D Perception Model with Persistent StateğŸ”¥** | CVPR | 2025 |
| [GitHub](https://github.com/Junyi42/monst3r) | [Paper](https://monst3r-project.github.io/files/monst3r_paper.pdf) | **MonST3R: A Simple Approach for Estimating Geometry in the Presence of MotionğŸ”¥** | ICLR | 2025 |
| [GitHub](https://github.com/facebookresearch/vggt) | [Paper](https://arxiv.org/abs/2503.11651) | **VGGT: Visual Geometry Grounded Transformer ğŸ”¥ğŸ”¥** | CVPR | 2025 |
| [GitHub](https://github.com/facebookresearch/map-anything) | [Paper](https://arxiv.org/abs/2509.13414) | **MapAnything: Universal Feed-Forward Metric 3D Reconstruction ğŸ”¥ğŸ”¥** | CVPR | 2025 |
| [GitHub](https://github.com/bb12346/geocomplete_codes) | [Paper](https://arxiv.org/abs/2510.03110) | **Geometry-Aware Diffusion for Reference-Driven Image Completion ğŸ”¥** | NeurIPS | 2025 |



### NeRF & 3DGS

| Code | Paper | Title | Venue | Year |
|------|-------|-------|-------|------|
| [GitHub](https://github.com/Harry-Zhi/semantic_nerf) | [Paper](https://arxiv.org/abs/2103.15875) | **Semantic-NeRF: Semantic Neural Radiance Fields** ğŸ”¥ | ICCV | 2021 |
| [GitHub](https://github.com/cassiePython/CLIPNeRF) | [Paper](https://arxiv.org/abs/2112.05139) | **CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields** | CVPR | 2022 |
| [GitHub](https://github.com/VITA-Group/GNT) | [Paper](https://arxiv.org/abs/2207.13298) | **Is Attention All That NeRF Needs?** |  ICLR | 2023 |
| [GitHub](https://github.com/zyqz97/GP-NeRF) | [Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_GP-NeRF_Generalized_Perception_NeRF_for_Context-Aware_3D_Scene_Understanding_CVPR_2024_paper.pdf) | **GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene Understanding** | CVPR | 2023 |
| [GitHub](https://github.com/JenningsL/nerf-loc) | [Paper](https://arxiv.org/abs/2304.07979) | **NeRF-Loc: Visual Localization with Conditional Neural Radiance Field** | ICRA | 2023 |
| [GitHub](https://github.com/chungmin99/garfield) | [Paper](https://arxiv.org/abs/2401.09419) | **GARField: Group Anything with Radiance FieldsğŸ”¥** | CVPR | 2024 |
| [GitHub](https://github.com/zubair-irshad/NeRF-MAE) | [Paper](https://arxiv.org/abs/2404.01300) | **NeRF-MAE : Masked AutoEncoders for Self-Supervised 3D Representation Learning for Neural Radiance FieldsğŸ”¥** | ECCV | 2024 |
| [GitHub](https://github.com/DP-Recon/DP-Recon) | [Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Ni_Decompositional_Neural_Scene_Reconstruction_with_Generative_Diffusion_Prior_CVPR_2025_paper.pdf) | **Decompositional Neural Scene Reconstruction with Generative Diffusion Prior ğŸ”¥** | CVPR | 2025 |
| [GitHub](https://github.com/mRobotit/Cues3D) | [Paper](https://www.sciencedirect.com/science/article/pii/S1566253525002374) | **Cues3D: Unleashing the power of sole NeRF for consistent and unique 3D instance segmentation** | Information Fusion | 2025 |
|------|------|-------|------|-----|
| [GitHub](https://github.com/minghanqin/LangSplat) | [Paper](https://arxiv.org/pdf/2312.16084) | **LangSplat: 3D Language Gaussian Splatting ğŸ”¥ğŸ”¥** | CVPR | 2024 |
| [GitHub](https://github.com/TQTQliu/MVSGaussian) | [Paper](https://arxiv.org/abs/2405.12218) | **MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from Multi-View StereoğŸ”¥** | ECCV | 2024 |
| [GitHub](https://github.com/ShijieZhou-UCLA/feature-3dgs) | [Paper](https://arxiv.org/abs/2312.03203) | **Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields ğŸ”¥ğŸ”¥** | CVPR | 2024 |
| [GitHub](https://github.com/Runsong123/Unified-Lift) | [Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_Rethinking_End-to-End_2D_to_3D_Scene_Segmentation_in_Gaussian_Splatting_CVPR_2025_paper.pdf) | **Unified-Lift: Rethinking End-to-End 2D to 3D Scene Segmentation in Gaussian Splatting ğŸ”¥** | CVPR | 2025 |
| [GitHub](https://any3dis.github.io/) | [Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Nguyen_Any3DIS_Class-Agnostic_3D_Instance_Segmentation_by_2D_Mask_Tracking_CVPR_2025_paper.pdf) | **Any3DIS: Class-Agnostic 3D Instance Segmentation by 2D Mask Tracking ğŸ”¥** | CVPR | 2025 |
| [GitHub](https://github.com/zhaihongjia/PanoGS) | [Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhai_PanoGS_Gaussian-based_Panoptic_Segmentation_for_3D_Open_Vocabulary_Scene_Understanding_CVPR_2025_paper.pdf) | **PanoGS: Gaussian-based Panoptic Segmentation for 3D Open Vocabulary Scene Understanding ğŸ”¥** | CVPR | 2025 |
| [GitHub](https://github.com/xingyoujun/transplat) | [Paper](https://arxiv.org/abs/2408.13770) | **TranSplat: Generalizable 3D Gaussian Splatting from Sparse Multi-View Images with Transformers** | AAAI | 2025 |
| [GitHub](https://github.com/ChenYutongTHU/SplatFormer) | [Paper](https://openreview.net/pdf?id=9NfHbWKqMF) | **SplatFormer: Point Transformer for Robust 3D Gaussian Splatting** | ICLR | 2025 |


















## Images Representation Learning

| Code | Paper | Title | Venue | Year |
|------|-------|-------|-------|------|
| [Study](https://fengzhiheng.github.io/2018/03/21/%E5%B0%BA%E5%BA%A6%E4%B8%8D%E5%8F%98%E7%89%B9%E5%BE%81%E5%8F%98%E6%8D%A2-SIFT/) | [Paper](https://www.cs.ubc.ca/~lsigal/425_2024W1/ijcv04.pdf) | **Distinctive Image Features from Scale-Invariant Keypoints** | IJCV | 2004 |
| [Study](https://zhuanlan.zhihu.com/p/679983274) | [Paper](https://ieeexplore.ieee.org/abstract/document/1467360) | **Histograms of oriented gradients for human detection** | CVPR | 2005 |
| [Study](https://blog.csdn.net/weixin_43334693/article/details/128127653) | [Paper](https://dl.acm.org/doi/pdf/10.1145/3065386) | **ImageNet Classification with Deep Convolutional Neural Networks** | NeurIPS | 2012 |
| [Study](https://blog.csdn.net/m0_63161039/article/details/147591529) | [Paper](https://arxiv.org/abs/1409.1556) | **Very Deep Convolutional Networks for Large-Scale Image Recognition** | CVPR | 2014 |
| [Study](https://blog.csdn.net/weixin_44070509/article/details/118862628) | [Paper](https://arxiv.org/abs/2003.08934) | **Deep Residual Learning for Image Recognition** | CVPR | 2016 |
| [Study](https://hezephyr.github.io/posts/04.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0transformer/) | [Paper](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) | **Attention Is All You Need** | NeurIPS | 2017 |
| [Study](https://blog.csdn.net/weixin_42426841/article/details/144542662) | [Paper](https://arxiv.org/abs/1810.04805) | **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** | Arxiv | 2018 |
| [GitHub](https://github.com/google-research/vision_transformer) | [Paper](https://arxiv.org/abs/2010.11929) | **An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale** | CVPR | 2020 |
| [GitHub](https://github.com/microsoft/Swin-Transformer) | [Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf) | **Swin Transformer: Hierarchical Vision Transformer using Shifted Windows** | CVPR | 2021 |
| [GitHub](https://github.com/isl-org/DPT) | [Paper](https://arxiv.org/pdf/2103.13413) | **Vision Transformers for Dense Prediction** | TPAMI | 2021 |
| [GitHub](https://github.com/microsoft/unilm/tree/master/beit) | [Paper](https://arxiv.org/pdf/2106.08254) | **BEIT: BERTPre-Training of Image Transformers Natural Language Supervision** | ICML | 2021 |
| [GitHub](https://github.com/microsoft/unilm/tree/master/beit2) | [Paper](https://arxiv.org/abs/2208.06366) | **BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers** | ICML | 2021 |
| [GitHub](https://github.com/microsoft/unilm/tree/master/beit3) | [Paper](https://arxiv.org/abs/2208.10442) | **(BEiT-3) Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks** | cvpr | 2023 |
| [GitHub](https://github.com/facebookresearch/moco) | [Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf) | **MoCo: Momentum Contrast for Unsupervised Visual Representation Learning** | ICLR | 2022 |
| [GitHub](https://github.com/OpenAI/CLIP) | [Paper](https://proceedings.mlr.press/v139/radford21a/radford21a.pdf) | **Learning Transferable Visual Models From Natural Language Supervision** | ICML | 2021 |
| [GitHub](https://github.com/facebookresearch/segment-anything) | [Paper](https://scontent-cdg4-2.xx.fbcdn.net/v/t39.2365-6/10000000_900554171201033_1602411987825904100_n.pdf?_nc_cat=100&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=_2xwUxCGtOwQ7kNvwE7m9_M&_nc_oc=Adnp99DAAA_5TC-txI7pHL0al0dCH4LrLLhalZCpRcXnQUIRM-5TXrjL7E1uJDRaetk&_nc_zt=14&_nc_ht=scontent-cdg4-2.xx&_nc_gid=_npAfmYncKTrxZL_OAExjg&oh=00_AffXwtnPTnjh48ZL9mZebSnhYFLgSpPejhu74w6fHgMOtw&oe=68EA6C67) | **Segment Anything** | ICCV | 2023 |
| [GitHub](https://github.com/facebookresearch/sam2) | [Paper](https://arxiv.org/pdf/2408.00714) | **SAM 2: Segment Anything in Images and Videos** | Arxiv | 2024 |
| [GitHub](https://github.com/facebookresearch/dino) | [Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.pdf) | **Emerging Properties in Self-Supervised Vision Transformers** | ICCV | 2021 |
| [GitHub](https://github.com/facebookresearch/dinov2) | [Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Jose_DINOv2_Meets_Text_A_Unified_Framework_for_Image-_and_Pixel-Level_CVPR_2025_paper.pdf) | **DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment** | CVPR | 2025 |
| [GitHub](https://github.com/facebookresearch/dinov3) | [Paper](https://arxiv.org/abs/2508.10104) | **DINOv3** | Arxiv | 2025 |








## Study  
[[è¡¨å¾å­¦ä¹ ]](https://www.jiqizhixin.com/graph/technologies/64d4c374-6061-46cc-8d29-d0a582934876)  
[[ä»€ä¹ˆæ˜¯Representation Learning]](https://zhuanlan.zhihu.com/p/136554341)  

[[Feature engineering]](https://en.wikipedia.org/wiki/Feature_engineering)  
[[Feature learning]](https://en.wikipedia.org/wiki/Feature_learning)  

[[ç›‘ç£å­¦ä¹ (Supervised) vs æ— ç›‘ç£å­¦ä¹ (Unsupervised) vs åŠç›‘ç£/è‡ªç›‘ç£å­¦ä¹ (Semi/Self-supervised)è¯¦è§£]](https://blog.csdn.net/ai_aijiang/article/details/148883481)  
[[Supervised-learning]](https://en.wikipedia.org/wiki/Supervised_learning)  
[[Unsupervised-learning]](https://en.wikipedia.org/wiki/Unsupervised_learning)  
[[éç›‘ç£å­¦ä¹ (Unsupervised Learningï¼ŒUL)]](https://blog.csdn.net/weixin_55629186/article/details/135109805)  
[[Self-supervised learning]](https://en.wikipedia.org/wiki/Self-supervised_learning)  
[[Self-Supervised Learning å…¥é—¨ä»‹ç»]](https://zhuanlan.zhihu.com/p/108625273)  
[[Self-supervised Learning å†æ¬¡å…¥é—¨]](https://zhuanlan.zhihu.com/p/108906502)  
[[ä»ViTåˆ°CLIPåˆ°DINOï¼šè§†è§‰è¡¨å¾å­¦ä¹ çš„æ¼”è¿›]](https://zhuanlan.zhihu.com/p/19842773699)  

[[ä¸€æ–‡ææ‡‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„åŸç†]](https://blog.csdn.net/AI_dataloads/article/details/133250229)
[[ä¸€æ–‡è¯»æ‡‚Transformer]](https://blog.csdn.net/weixin_42475060/article/details/121101749)  
[[Transformeræ¨¡å‹è¯¦è§£]](https://zhuanlan.zhihu.com/p/338817680)  
[[æ·±åº¦è§†è§‰è¡¨å¾å­¦ä¹ ]](https://ustc-dia.github.io/slides_2025_spring/%E7%AC%AC4%E7%AB%A0-%E5%9B%BE%E5%83%8F%E8%A1%A8%E8%BE%BE-4.4-%E6%B7%B1%E5%BA%A6%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0.pdf)  
[[From Autoencoder to Beta-VAE]](https://lilianweng.github.io/posts/2018-08-12-vae/)  
[[Knowledge Distillation]](https://blog.csdn.net/weixin_43694096/article/details/127505946)  
[[çŸ¥è¯†è’¸é¦æŠ€æœ¯(æ•™å¸ˆå­¦ç”Ÿæ¨¡å‹)]](https://zhuanlan.zhihu.com/p/442457531)  
[[è¯¦è§£æ·±åº¦å­¦ä¹ ä¸­çš„æ•™å¸ˆ-å­¦ç”Ÿæ¨¡å‹(Teacher- Student Model)]](https://blog.csdn.net/Shirelle_/article/details/136572339)  
[[ä¸€è‡´æ€§æ­£åˆ™åŒ–]](https://blog.csdn.net/by6671715/article/details/123087728)  
[[åŠç›‘ç£å­¦ä¹ æ–¹æ³•: ä¸€è‡´æ€§æ­£åˆ™åŒ–]](https://blog.csdn.net/by6671715/article/details/123087728)  
[[ä¸€æ–‡è¯»æ‡‚ èšç±»ç‰¹å¾é€‰æ‹©]](https://www.jianshu.com/p/f24a9ad30738)  
[[æ·±åº¦èšç±»]](https://blog.csdn.net/allein_STR/article/details/128569765)  
[[ä¸€æ–‡ç†è§£å˜åˆ†è‡ªç¼–ç å™¨(VAE)]](https://zhuanlan.zhihu.com/p/64485020)  
[[å˜åˆ†è‡ªç¼–ç å™¨(Variational Autoencodersï¼ŒVAE)è¯¦è§£ï¼šæ•°å­¦åŸç†ã€å›¾ç¤ºã€ä»£ç ]](https://www.vectorexplore.com/tech/auto-encoder/vae.html)  
[[ä¸€æ–‡å½»åº•ææ‡‚å‰å‘ä¼ æ’­(Forward Pass)ä¸åå‘ä¼ æ’­(Backward Pass)]](https://blog.csdn.net/Zlyzjiabjw547479/article/details/149138272)  

[[æ— ç›‘ç£å­¦ä¹ : åŠ¨é‡å¯¹æ¯”(MoCO)è®ºæ–‡ç¬”è®°]](https://zhuanlan.zhihu.com/p/102573476)  
[[MoCOè®ºæ–‡è§£è¯»]](https://www.bilibili.com/video/BV19S4y1M7hm/)  
[[å¯¹æ¯”å­¦ä¹ çš„æ·±å…¥æŒ‡å—ï¼šæŠ€æœ¯ã€æ¨¡å‹å’Œåº”ç”¨]](https://www.myscale.com/blog/zh/what-is-contrastive-learning/)  
[[CLIPè®ºæ–‡ç¬”è®°]](https://zhuanlan.zhihu.com/p/579493763)  
[[CLIPè®ºæ–‡è§£è¯»]](https://www.bilibili.com/video/BV1SL4y1s7LQ)  
[[VITè®ºæ–‡ç²¾è¯»]](https://blog.csdn.net/abc13526222160/article/details/131228810)  
[[VITä»£ç è¯¦è§£]](https://blog.csdn.net/qq_39478403/article/details/118704747)  


 
