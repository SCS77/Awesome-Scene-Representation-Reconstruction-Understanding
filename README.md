<h1 align="center">
  <b>Scene-Reconstruction-Understanding </b>
</h1>

ðŸ“¦ Curated collection of papers, datasets, codebases, and resources in the field of scene reconstruction and understanding using NeRF and 3DGS.

## ðŸ˜„ðŸ˜„ <span style="color:red;">Under Construction</span>  ðŸ˜„ðŸ˜„


## [ðŸ”¥ Scene Reconstruction](#scene-reconstruction)  

### â€¢ [NeRF(Neural Radiance Fields)](#nerf) 
### â€¢ [3DGS(3D Gaussian Splatting)](#3dgs)  


## [ðŸš€ Scene Understanding](#scene-understanding)  


---


## Scene Reconstruction

### NeRF & 3DGS

| Code | Paper | Title | Venue | Year |
|------|-------|-------|-------|------|
| [GitHub](https://github.com/bmild/nerf) | [Paper](https://arxiv.org/abs/2003.08934) | **NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis ðŸ”¥ðŸ”¥** | ECCV | 2020 |
| [GitHub](https://github.com/nerfstudio-project/nerfstudio) | ------ | **nerfstudio ðŸ”¥ðŸ”¥** | ---- | 2022 |
| [GitHub](https://github.com/Xharlie/pointnerf) | [Paper](https://arxiv.org/pdf/2201.08845) | **Point-NeRF: Point-based Neural Radiance Fields ðŸ”¥ðŸ”¥** | CVPR | 2020 |
| [GitHub](https://github.com/NVlabs/instant-ngp) | [Paper](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf) | **Instant Neural Graphics Primitives with a Multiresolution Hash Encoding ðŸ”¥ðŸ”¥** | TOG | 2022 |
|------|-------|-------|-------|------|
| [GitHub](https://github.com/graphdeco-inria/gaussian-splatting) | [Paper](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/3d_gaussian_splatting_high.pdf) | **3D Gaussian Splatting for Real-Time Radiance Field Rendering ðŸ”¥ðŸ”¥** | TOG | 2023 |
| [GitHub](https://github.com/zju3dv/EasyVolcap) | [Paper](https://drive.google.com/file/d/1J9Hl7KPMOkBmrt6UTFf_u9sQcjbblILA/view) | **Representing Long Volumetric Video with Temporal Gaussian Hierarchy ðŸ”¥ðŸ”¥** | TOG | 2024 |
| [GitHub](https://github.com/hbb1/2d-gaussian-splatting) | [Paper](https://arxiv.org/abs/2403.17888) | **2D Gaussian Splatting for Geometrically Accurate Radiance Fields ðŸ”¥ðŸ”¥** | SIGGRAPH | 2024 |
| [GitHub](https://github.com/ingra14m/Deformable-3D-Gaussians) | [Paper](https://arxiv.org/abs/2309.13101) | **Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction ðŸ”¥ðŸ”¥** | CVPR | 2024 |
| [GitHub](https://github.com/hustvl/4DGaussians) | [Paper](https://arxiv.org/pdf/2310.08528v2) | **4D Gaussian Splatting for Real-Time Dynamic Scene Rendering ðŸ”¥ðŸ”¥** | CVPR | 2024 |




## Scene understanding

### Geometry-based and Representation Learning 

| Code | Paper | Title | Venue | Year |
|------|-------|-------|-------|------|
| [GitHub](https://github.com/google-research/scenic) | [Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Yan_Multiview_Transformers_for_Video_Recognition_CVPR_2022_paper.pdf) | **Multiview Transformers for Video Recognition ðŸ”¥ðŸ”¥** | CVPR | 2022 |  
| [GitHub](https://github.com/bradyz/cross_view_transformers) | [Paper](https://arxiv.org/abs/2205.02833) | **Cross-view Transformers for real-time Map-view Semantic Segmentation** | CVPR | 2022 |  
| [GitHub](https://github.com/chtsy/buol) | [Paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Chu_BUOL_A_Bottom-Up_Framework_With_Occupancy-Aware_Lifting_for_Panoptic_3D_CVPR_2023_paper.pdf) | **BUOL: A Bottom-Up Framework with Occupancy-aware Lifting for Panoptic 3D Scene Reconstruction From A Single Image** | CVPR | 2023 |
| [GitHub](https://github.com/megvii-research/TransMVSNet) | [Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_TransMVSNet_Global_Context-Aware_Multi-View_Stereo_Network_With_Transformers_CVPR_2022_paper.pdf) | **TransMVSNet: Global Context-aware Multi-view Stereo Network with Transformers ðŸ”¥** | CVPR | 2022 |
| [GitHub](https://github.com/dcharatan/flowmap) | [Paper](https://arxiv.org/abs/2404.15259) | **FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient Descent** ðŸ”¥ | arXiv | 2024 |
| [GitHub](https://github.com/facebookresearch/vggsfm) | [Paper](https://arxiv.org/pdf/2312.04563) | **VGGSfM: Visual Geometry Grounded Deep Structure From MotionðŸ”¥** | CVPR | 2024 |
| [GitHub](https://github.com/Parskatt/RoMa) | [Paper](https://arxiv.org/abs/2305.15404) | **RoMa:Robust Dense Feature Matching** | CVPR | 2024 |
| [GitHub](https://github.com/naver/dust3r) | [Paper](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_DUSt3R_Geometric_3D_Vision_Made_Easy_CVPR_2024_paper.html) | **DUSt3R: Geometric 3D Vision Made Easy ðŸ”¥ðŸ”¥** | CVPR | 2024 |
| [GitHub](https://github.com/naver/must3r) | [Paper](https://openaccess.thecvf.com/content/CVPR2025/html/Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction_CVPR_2025_paper.html) | **MUSt3R: Multi-view Network for Stereo 3D Reconstruction ðŸ”¥** | CVPR | 2025 |
| [GitHub](https://github.com/naver/pow3r) | [Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Jang_Pow3R_Empowering_Unconstrained_3D_Reconstruction_with_Camera_and_Scene_Priors_CVPR_2025_paper.pdf#:~:text=In%20this%20paper%2C%20we%20introduce%20Pow3R%2C%20a%20new,sparse%20or%20dense%20depth%2C%20or%20relative%20camera%20poses.) | **Pow3R: Empowering Unconstrained 3D  Reconstruction with Camera and Scene Priors** | CVPR | 2025 |
| [GitHub](https://github.com/facebookresearch/mvdust3r) | [Paper](https://arxiv.org/abs/2412.06974) | **MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds** | CVPR | 2025 |
| [GitHub](https://github.com/jiah-cloud/Align3R) | [Paper](https://arxiv.org/abs/2412.03079) | **Align3R: Aligned Monocular Depth Estimation for Dynamic Videos** | CVPR | 2025 |
| [GitHub](https://github.com/HKUST-SAIL/sail-recon) | [Paper](https://arxiv.org/abs/2508.17972) | **SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization** | Arxiv | 2025 |
| [GitHub](https://github.com/CUT3R/CUT3R) | [Paper](https://arxiv.org/pdf/2501.12387) | **Continuous 3D Perception Model with Persistent StateðŸ”¥** | CVPR | 2025 |
| [GitHub](https://github.com/Junyi42/monst3r) | [Paper](https://monst3r-project.github.io/files/monst3r_paper.pdf) | **MonST3R: A Simple Approach for Estimating Geometry in the Presence of MotionðŸ”¥** | ICLR | 2025 |
| [GitHub](https://github.com/facebookresearch/fast3r) | [Paper](https://arxiv.org/abs/2501.13928) | **Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward PassðŸ”¥** | CVPR | 2025 |
| [GitHub](https://github.com/facebookresearch/vggt) | [Paper](https://arxiv.org/abs/2503.11651) | **VGGT: Visual Geometry Grounded Transformer ðŸ”¥ðŸ”¥** | CVPR | 2025 |
| [GitHub](https://github.com/facebookresearch/map-anything) | [Paper](https://arxiv.org/abs/2509.13414) | **MapAnything: Universal Feed-Forward Metric 3D Reconstruction ðŸ”¥ðŸ”¥** | CVPR | 2025 |
| [GitHub](https://github.com/bb12346/geocomplete_codes) | [Paper](https://arxiv.org/abs/2510.03110) | **Geometry-Aware Diffusion for Reference-Driven Image Completion ðŸ”¥** | NeurIPS | 2025 |
| [GitHub](https://github.com/yyfz/Pi3) | [Paper](https://arxiv.org/abs/2507.13347) | **Ï€Â³: Permutation-Equivariant Visual Geometry Learning ðŸ”¥ðŸ”¥** | arXiv | 2025 |
| [GitHub](https://github.com/Seed3D/Dora) | [Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Dora_Sampling_and_Benchmarking_for_3D_Shape_Variational_Auto-Encoders_CVPR_2025_paper.pdf) | **Dora: Sampling and Benchmarking for 3D Shape Variational Auto-Encoders** | CVPR | 2025 |
| [GitHub](https://github.com/WU-CVGL/SIU3R) | [Paper](https://arxiv.org/abs/2507.02705) | **SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment** | NeurIPS | 2025 |







### NeRF & 3DGS  

| Code | Paper | Title | Venue | Year |
|------|-------|-------|-------|------|
| [GitHub](https://github.com/Harry-Zhi/semantic_nerf) | [Paper](https://arxiv.org/abs/2103.15875) | **Semantic-NeRF: Semantic Neural Radiance Fields** ðŸ”¥ | ICCV | 2021 |
| [GitHub](https://github.com/cassiePython/CLIPNeRF) | [Paper](https://arxiv.org/abs/2112.05139) | **CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields** | CVPR | 2022 |
| [GitHub](https://github.com/VITA-Group/GNT) | [Paper](https://arxiv.org/abs/2207.13298) | **Is Attention All That NeRF Needs?** |  ICLR | 2023 |
| [GitHub](https://github.com/zyqz97/GP-NeRF) | [Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_GP-NeRF_Generalized_Perception_NeRF_for_Context-Aware_3D_Scene_Understanding_CVPR_2024_paper.pdf) | **GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene Understanding** | CVPR | 2023 |
| [GitHub](https://github.com/JenningsL/nerf-loc) | [Paper](https://arxiv.org/abs/2304.07979) | **NeRF-Loc: Visual Localization with Conditional Neural Radiance Field** | ICRA | 2023 |
| [GitHub](https://github.com/chungmin99/garfield) | [Paper](https://arxiv.org/abs/2401.09419) | **GARField: Group Anything with Radiance FieldsðŸ”¥** | CVPR | 2024 |
| [GitHub](https://github.com/zubair-irshad/NeRF-MAE) | [Paper](https://arxiv.org/abs/2404.01300) | **NeRF-MAE : Masked AutoEncoders for Self-Supervised 3D Representation Learning for Neural Radiance FieldsðŸ”¥** | ECCV | 2024 |
| [GitHub](https://github.com/DP-Recon/DP-Recon) | [Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Ni_Decompositional_Neural_Scene_Reconstruction_with_Generative_Diffusion_Prior_CVPR_2025_paper.pdf) | **Decompositional Neural Scene Reconstruction with Generative Diffusion Prior ðŸ”¥** | CVPR | 2025 |
| [GitHub](https://github.com/mRobotit/Cues3D) | [Paper](https://www.sciencedirect.com/science/article/pii/S1566253525002374) | **Cues3D: Unleashing the power of sole NeRF for consistent and unique 3D instance segmentation** | Information Fusion | 2025 |
| ----- | [Paper](https://arxiv.org/abs/2509.25191) | **VGGT-X: When VGGT Meets Dense Novel View Synthesis** | Arxiv | 2025 |
|------|------|-------|------|-----|
| [GitHub](https://github.com/minghanqin/LangSplat) | [Paper](https://arxiv.org/pdf/2312.16084) | **LangSplat: 3D Language Gaussian Splatting ðŸ”¥ðŸ”¥** | CVPR | 2024 |
| [GitHub](https://github.com/TQTQliu/MVSGaussian) | [Paper](https://arxiv.org/abs/2405.12218) | **MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from Multi-View StereoðŸ”¥** | ECCV | 2024 |
| [GitHub](https://github.com/dcharatan/pixelsplat) | [Paper](https://arxiv.org/abs/2312.12337) | **pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction** | ECCV | 2024 |
| [GitHub](https://github.com/AbrahamYabo/LiftImage3D) | [Paper](https://arxiv.org/pdf/2412.09597) | **LiftImage3D: Lifting Any Single Image to 3D Gaussians with Video Generation Priors** | Arxiv | 2024 |
| [GitHub](https://github.com/ShijieZhou-UCLA/feature-3dgs) | [Paper](https://arxiv.org/abs/2312.03203) | **Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields ðŸ”¥ðŸ”¥** | CVPR | 2024 |
| [GitHub](https://github.com/huang-yh/GaussianFormer) | [Paper](https://arxiv.org/abs/2405.17429) | **GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy PredictionðŸ”¥** | ECCV | 2024 |
| [GitHub](https://github.com/huang-yh/GaussianFormer) | [Paper](https://arxiv.org/abs/2412.04384) | **GaussianFormer-2: Probabilistic Gaussian Superposition for Efficient 3D Occupancy PredictionðŸ”¥** | CVPR | 2025 |
| [GitHub](https://github.com/Runsong123/Unified-Lift) | [Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_Rethinking_End-to-End_2D_to_3D_Scene_Segmentation_in_Gaussian_Splatting_CVPR_2025_paper.pdf) | **Unified-Lift: Rethinking End-to-End 2D to 3D Scene Segmentation in Gaussian Splatting ðŸ”¥** | CVPR | 2025 |
| [GitHub](https://any3dis.github.io/) | [Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Nguyen_Any3DIS_Class-Agnostic_3D_Instance_Segmentation_by_2D_Mask_Tracking_CVPR_2025_paper.pdf) | **Any3DIS: Class-Agnostic 3D Instance Segmentation by 2D Mask Tracking ðŸ”¥** | CVPR | 2025 |
| [GitHub](https://github.com/zhaihongjia/PanoGS) | [Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhai_PanoGS_Gaussian-based_Panoptic_Segmentation_for_3D_Open_Vocabulary_Scene_Understanding_CVPR_2025_paper.pdf) | **PanoGS: Gaussian-based Panoptic Segmentation for 3D Open Vocabulary Scene Understanding ðŸ”¥** | CVPR | 2025 |
| [GitHub](https://github.com/xingyoujun/transplat) | [Paper](https://arxiv.org/abs/2408.13770) | **TranSplat: Generalizable 3D Gaussian Splatting from Sparse Multi-View Images with Transformers** | AAAI | 2025 |
| [GitHub](https://github.com/ChenYutongTHU/SplatFormer) | [Paper](https://openreview.net/pdf?id=9NfHbWKqMF) | **SplatFormer: Point Transformer for Robust 3D Gaussian Splatting** | ICLR | 2025 |
| [GitHub](https://github.com/rmurai0610/MASt3R-SLAM) | [Paper](https://arxiv.org/abs/2412.12392) | **MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors ðŸ”¥ðŸ”¥** | CVPR | 2025 |






## Images Representation Learning

| Code | Paper | Title | Venue | Year |
|------|-------|-------|-------|------|
| [Study](https://fengzhiheng.github.io/2018/03/21/%E5%B0%BA%E5%BA%A6%E4%B8%8D%E5%8F%98%E7%89%B9%E5%BE%81%E5%8F%98%E6%8D%A2-SIFT/) | [Paper](https://www.cs.ubc.ca/~lsigal/425_2024W1/ijcv04.pdf) | **Distinctive Image Features from Scale-Invariant Keypoints** | IJCV | 2004 |
| [Study](https://zhuanlan.zhihu.com/p/679983274) | [Paper](https://ieeexplore.ieee.org/abstract/document/1467360) | **Histograms of oriented gradients for human detection** | CVPR | 2005 |
| [Study](https://blog.csdn.net/weixin_43334693/article/details/128127653) | [Paper](https://dl.acm.org/doi/pdf/10.1145/3065386) | **ImageNet Classification with Deep Convolutional Neural Networks** | NeurIPS | 2012 |
| [Study](https://blog.csdn.net/m0_63161039/article/details/147591529) | [Paper](https://arxiv.org/abs/1409.1556) | **Very Deep Convolutional Networks for Large-Scale Image Recognition** | CVPR | 2014 |
| [Study](https://blog.csdn.net/weixin_44070509/article/details/118862628) | [Paper](https://arxiv.org/abs/2003.08934) | **Deep Residual Learning for Image Recognition** | CVPR | 2016 |
| [Study](https://hezephyr.github.io/posts/04.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0transformer/) | [Paper](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) | **Attention Is All You Need** | NeurIPS | 2017 |
| [Study](https://blog.csdn.net/weixin_42426841/article/details/144542662) | [Paper](https://arxiv.org/abs/1810.04805) | **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** | Arxiv | 2018 |
| [GitHub](https://github.com/google-research/vision_transformer) | [Paper](https://arxiv.org/abs/2010.11929) | **An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale** | CVPR | 2020 |
| [GitHub](https://github.com/microsoft/Swin-Transformer) | [Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf) | **Swin Transformer: Hierarchical Vision Transformer using Shifted Windows** | CVPR | 2021 |
| [GitHub](https://github.com/isl-org/DPT) | [Paper](https://arxiv.org/pdf/2103.13413) | **Vision Transformers for Dense Prediction** | TPAMI | 2021 |
| [GitHub](https://github.com/microsoft/unilm/tree/master/beit) | [Paper](https://arxiv.org/pdf/2106.08254) | **BEIT: BERTPre-Training of Image Transformers Natural Language Supervision** | ICML | 2021 |
| [GitHub](https://github.com/microsoft/unilm/tree/master/beit2) | [Paper](https://arxiv.org/abs/2208.06366) | **BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers** | ICML | 2021 |
| [GitHub](https://github.com/microsoft/unilm/tree/master/beit3) | [Paper](https://arxiv.org/abs/2208.10442) | **(BEiT-3) Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks** | cvpr | 2023 |
| [GitHub](https://github.com/facebookresearch/moco) | [Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf) | **MoCo: Momentum Contrast for Unsupervised Visual Representation Learning** | ICLR | 2022 |
| [GitHub](https://github.com/OpenAI/CLIP) | [Paper](https://proceedings.mlr.press/v139/radford21a/radford21a.pdf) | **Learning Transferable Visual Models From Natural Language Supervision** | ICML | 2021 |
| [GitHub](https://github.com/facebookresearch/segment-anything) | [Paper](https://scontent-cdg4-2.xx.fbcdn.net/v/t39.2365-6/10000000_900554171201033_1602411987825904100_n.pdf?_nc_cat=100&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=_2xwUxCGtOwQ7kNvwE7m9_M&_nc_oc=Adnp99DAAA_5TC-txI7pHL0al0dCH4LrLLhalZCpRcXnQUIRM-5TXrjL7E1uJDRaetk&_nc_zt=14&_nc_ht=scontent-cdg4-2.xx&_nc_gid=_npAfmYncKTrxZL_OAExjg&oh=00_AffXwtnPTnjh48ZL9mZebSnhYFLgSpPejhu74w6fHgMOtw&oe=68EA6C67) | **Segment Anything** | ICCV | 2023 |
| [GitHub](https://github.com/facebookresearch/sam2) | [Paper](https://arxiv.org/pdf/2408.00714) | **SAM 2: Segment Anything in Images and Videos** | Arxiv | 2024 |
| [GitHub](https://github.com/facebookresearch/dino) | [Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.pdf) | **Emerging Properties in Self-Supervised Vision Transformers** | ICCV | 2021 |
| [GitHub](https://github.com/facebookresearch/dinov2) | [Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Jose_DINOv2_Meets_Text_A_Unified_Framework_for_Image-_and_Pixel-Level_CVPR_2025_paper.pdf) | **DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment** | CVPR | 2025 |
| [GitHub](https://github.com/facebookresearch/dinov3) | [Paper](https://arxiv.org/abs/2508.10104) | **DINOv3** | Arxiv | 2025 |


