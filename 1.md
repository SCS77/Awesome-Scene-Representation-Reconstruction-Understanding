
# æ ¸å¿ƒæ–¹æ³•ç®€ä»‹ï¼šVGGT ä¸ MapAnything
-------------------------

è¿™ä¸¤ä¸ªæ–¹æ³•éƒ½å±äºâ€œå›¾åƒ â†’ è¡¨å¾ï¼ˆrepresentationï¼‰ â†’ åœºæ™¯å‡ ä½•é‡å»ºï¼æ¢å¤â€çš„ç±»åˆ«ï¼Œä½†æ–¹å‘å’Œè®¾è®¡é‡ç‚¹ä¸åŒã€‚

| æ–¹æ³• | æ ¸å¿ƒä»»åŠ¡ /è¾“å…¥ï¼è¾“å‡º | æ–¹æ³•æ¶æ„ /å…³é”®ç»„ä»¶ | ä¼˜ç‚¹ | å±€é™ï¼æŒ‘æˆ˜ |
| --- | --- | --- | --- | --- |
| **VGGT: Visual Geometry Grounded Transformer** [vgg-t.github.io+1](https://vgg-t.github.io/?utm_source=chatgpt.com) | è¾“å…¥ï¼šä» 1 åˆ°å¤šä¸ª RGB å›¾åƒï¼›è¾“å‡ºï¼šç›¸æœºå‚æ•° (intrinsic + extrinsic)ã€æ·±åº¦å›¾ã€ç‚¹å›¾ï¼ˆpoint mapï¼‰ã€3D ç‚¹è¿½è¸ªï¼ˆpoint tracksï¼‰ç­‰å¤šä¸ª 3D å±æ€§ã€‚ | ä½¿ç”¨ transformer æ¶æ„ï¼šå…ˆ patchify å›¾åƒï¼ˆç”¨ DINO åš patch ç‰¹å¾ï¼‰ï¼Œå† alternating çš„ attention å±‚ (frame-wise & global)ï¼Œå¹¶è®¾å¤šä¸ªå¤´(head)ï¼šä¸€ä¸ªå¤´é¢„æµ‹ç›¸æœºå¤´(camera head)ï¼Œä¸€ä¸ª DPTï¼ˆdense prediction transformerï¼‰å¤´é¢„æµ‹æ·±åº¦ï¼ç‚¹å›¾ï¼feature mapã€‚è®­ç»ƒåœ¨å¤§é‡å¸¦ 3D æ³¨é‡Šçš„æ•°æ®é›†ä¸Šã€‚ [ç‰›æ´¥å¤§å­¦æœºå™¨äººç ”ç©¶æ‰€+1](https://www.robots.ox.ac.uk/~vedaldi/assets/pubs/wang25vggt.pdf?utm_source=chatgpt.com) | **é€Ÿåº¦å¿«**ï¼ˆfeed-forwardï¼Œå°‘æˆ–æ— éœ€åå¤„ç†ï¼‰ï¼Œ**ç»Ÿä¸€æ€§å¼º**ï¼šä¸€ä¸ªæ¨¡å‹å¤„ç†å¤šä¸ªä»»åŠ¡ï¼å¤šä¸ªè§†å›¾æ•°æƒ…å½¢ã€‚è¡¨ç°ä¸ä¸“é—¨æ–¹æ³•ç›¸æ¯”éå¸¸æœ‰ç«äº‰åŠ›ã€‚é€‚åˆåœ¨å›¾åƒå¤šè§†ï¼å°‘è§†æƒ…å†µä¸‹éƒ½ç”¨ã€‚ | è™½ç„¶å¿«ä½†åœ¨æç«¯ç»†èŠ‚ï¼å‡ ä½•é«˜ç²¾åº¦ï¼ˆå¦‚éå¸¸ç»†å¾®æ›²ç‡ï¼åå°„ï¼åŠé€æ˜æè´¨ï¼å¤æ‚å…‰ç…§ï¼‰å¯èƒ½ä¸å¦‚éšå¼è¡¨ç¤º + ä¼˜åŒ–æ–¹æ³•ã€‚ä¹Ÿå¯èƒ½å¯¹è§†è§’åˆ†å¸ƒï¼å›¾åƒè¦†ç›–ç‡æ•æ„Ÿã€‚å¯¹äºæ¸²æŸ“è´¨é‡ï¼å¤–è§‚ç»†èŠ‚ï¼ˆä¾‹å¦‚çº¹ç†ç»†èŠ‚æˆ–å¤–è§‚ä¸€è‡´æ€§ï¼‰å¯èƒ½ä¸å¦‚ NeRF/3DGS é‚£äº›ä¸“é—¨ä¼˜åŒ–çš„æ¨¡å‹ã€‚ |
| **MapAnything: Universal Feed-Forward Metric 3D Reconstruction** [Map Anything+1](https://map-anything.github.io/?utm_source=chatgpt.com) | è¾“å…¥ï¼šå›¾åƒï¼ˆå¯èƒ½ä¸€ä¸ªã€å¤šä¸ªï¼‰ï¼Œå¯é€‰çš„å‡ ä½•è¾“å…¥ï¼ˆå†…å‚ï¼æ·±åº¦ï¼ç›¸æœºå§¿æ€ï¼éƒ¨åˆ†é‡å»ºç­‰ï¼‰ï¼›è¾“å‡ºï¼šfactored metric 3D å‡ ä½•ï¼ˆdepth maps, local ray maps, ç›¸æœºä½å§¿/poses, scale factorç­‰ï¼‰ï¼Œæ ‡å®šå‡ ä½•ä»»åŠ¡ç»Ÿä¸€ã€‚ | Transformer æ¶æ„ + å¤šæ¨¡æ€ï¼å¤šè¾“å…¥ç±»å‹æ”¯æŒï¼›ä½¿ç”¨å›¾åƒ tokens + å‡ ä½•è¾“å…¥ tokensï¼›è¾“å‡ºè¢« factorizeï¼ˆåˆ†è§£æˆ depth maps + local rays + poses + scaleï¼‰ä»¥ä¿æŒ metric consistencyï¼›å¼ºç›‘ç£ +è·¨æ•°æ®é›†è®­ç»ƒã€‚ [Map Anything](https://map-anything.github.io/?utm_source=chatgpt.com) | é€šç”¨æ€§å¼ºï¼Œå¯¹å„ç§å‡ ä½•ä»»åŠ¡éƒ½èƒ½åšï¼›ä¸ä¾èµ–è¿‡å¤šæ‰‹å·¥ä¼˜åŒ–ï¼›åœ¨å¤šä¸ªä»»åŠ¡ï¼ˆSfM, MVS, ç›¸æœºå®šä½, å•ç›®æ·±åº¦ä¼°è®¡ç­‰ï¼‰ä¸ŠåŒ¹é…æˆ–ä¼˜äºä¸“é—¨æ–¹æ³•ã€‚å…¼å®¹ä¸åŒè¾“å…¥ç±»å‹ï¼Œæ˜¯è¾ƒä¸ºâ€œé€šç”¨â€çš„å‡ ä½•æ¢å¤ backboneã€‚ | ä¸ VGGT ç±»ä¼¼ï¼Œåœ¨é«˜è´¨é‡å¤–è§‚æ¸²æŸ“ï¼å…‰ç…§ï¼æè´¨ï¼ç¨ å¯†å¤–è§‚ä¸€è‡´æ€§æ–¹é¢å¯èƒ½ä¸å¦‚ implicit/rendering ä¼˜åŒ–æ¨¡å‹ã€‚ä¹Ÿå¯èƒ½å¯¹äºè§†å›¾ç¨€ç–ï¼é®æŒ¡ï¼åå°„ï¼åŠé€æ˜æè´¨ç­‰åœºæ™¯å…·æœ‰æŒ‘æˆ˜ã€‚Scaleï¼ç»†èŠ‚ç²¾åº¦å¯èƒ½å—åˆ°é™åˆ¶ã€‚è®­ç»ƒéœ€æ±‚å¤§ï¼ˆå¤šæ•°æ®é›†ã€æ³¨é‡Šã€å„ç§å‡ ä½•è¾“å…¥ï¼‰å¯¹äºæŸäº›åº”ç”¨å¯èƒ½ä»£ä»·é«˜ã€‚ |

* * *

## Prompt:
æ€»ç»“DUSt3Rã€MUSt3Rã€MonST3Rã€VGGTã€MapAnythingç­‰æ–¹æ³•å…±åŒçš„å®ç°é€»è¾‘å’Œç‰¹å¾

## Response:
éå¸¸å¥½çš„é—®é¢˜ã€‚  
ä½ æåˆ°çš„è¿™äº›æ–¹æ³•â€”â€”**DUSt3Rã€MUSt3Rã€MonST3Rã€VGGTã€MapAnything**â€”â€”å®é™…ä¸Šä»£è¡¨äº†è¿‡å»ä¸¤ä¸‰å¹´â€œå›¾åƒè¡¨å¾å­¦ä¹ é©±åŠ¨çš„å‡ ä½•ç†è§£ä¸åœºæ™¯é‡å»ºâ€çš„ä¸€ä¸ªæ ¸å¿ƒè°±ç³»ã€‚

å®ƒä»¬è™½ç„¶åå­—ä¸åŒï¼ˆ-3R ç³»åˆ—ã€VGGTã€MapAnythingï¼‰ï¼Œä½†**æœ‰ç€é«˜åº¦ä¸€è‡´çš„å®ç°é€»è¾‘ä¸è®¾è®¡å“²å­¦**ã€‚  
ä¸‹é¢æˆ‘ç»™ä½ ä¸€ä¸ªéå¸¸è¯¦ç»†ã€ç»“æ„åŒ–çš„æ€»ç»“ï¼Œä»åº•å±‚é€»è¾‘åˆ°è®­ç»ƒæœºåˆ¶ã€è¾“å‡ºè®¾è®¡ã€ä¼˜ç¼ºç‚¹ä¸å…±æ€§æœ¬è´¨åˆ†æã€‚

* * *

# ğŸ§­ ä¸€ã€æ€»ä½“æ€æƒ³ï¼šä»â€œä¼˜åŒ–â€åˆ°â€œå‰é¦ˆå‡ ä½•ç†è§£â€
========================

ä¼ ç»Ÿçš„ SfMã€MVSã€NeRFã€Bundle Adjustment éœ€è¦ï¼š

*   å¤šå¼ å›¾åƒè¾“å…¥ï¼›
*   é€šè¿‡åå¤çš„**å‡ ä½•ä¼˜åŒ–**ï¼ˆBAã€å…‰åº¦ä¸€è‡´æ€§ä¼˜åŒ–ç­‰ï¼‰ï¼›
*   æ‰èƒ½å¾—åˆ° 3D ç‚¹äº‘ã€ç›¸æœºä½å§¿ã€æ·±åº¦ç­‰ç»“æœã€‚

è¿™äº›æ–°æ–¹æ³•ï¼ˆDUSt3R â†’ MUSt3R â†’ MonST3R â†’ VGGT â†’ MapAnythingï¼‰å…±åŒçš„ç†å¿µæ˜¯ï¼š

> **ç”¨å­¦ä¹ åˆ°çš„å›¾åƒè¡¨å¾ç›´æ¥é¢„æµ‹å‡ ä½•ä¸ç›¸æœºå‚æ•°ï¼Œè·³è¿‡ä¼˜åŒ–ï¼Œç”¨å‰é¦ˆï¼ˆfeed-forwardï¼‰æ–¹å¼å®Œæˆåœºæ™¯ç†è§£ä¸é‡å»ºã€‚**

æ¢å¥è¯è¯´ï¼š

*   å®ƒä»¬ä¸å†æ˜¾å¼ä¼˜åŒ– 3D ç‚¹ï¼›
*   è€Œæ˜¯å­¦ä¹ åˆ°ä¸€ä¸ª transformer ç½‘ç»œï¼Œå°†å›¾åƒç¼–ç æˆä¸€ä¸ªå…¨å±€ä¸€è‡´çš„å‡ ä½•è¡¨å¾ï¼›
*   ç„¶åä¸€æ¬¡å‰å‘æ¨ç†ï¼Œå°±è¾“å‡º 3D ç‚¹äº‘ã€ç›¸æœºå§¿æ€ã€æ·±åº¦å›¾ã€ç‚¹å¯¹åº”å…³ç³»ç­‰ã€‚

* * *

# ğŸ§© äºŒã€ç»Ÿä¸€çš„å®ç°é€»è¾‘
============

| æ¨¡å— | å…±æ€§é€»è¾‘ | æŠ€æœ¯ç»†èŠ‚ï¼ˆå‡ ä¹æ‰€æœ‰æ–¹æ³•å…±äº«ï¼‰ |
| --- | --- | --- |
| **è¾“å…¥** | 1ï½N å¼ å›¾åƒï¼ˆé€šå¸¸å¸¦å†…å‚ï¼Œæœ‰æ—¶ä¸å¸¦ï¼‰ | å•ç›® / åŒç›® / å¤šè§†å›¾å‡å¯ã€‚DUSt3R æ˜¯ä¸¤å¼ å›¾ï¼ŒMUSt3R/MonST3R æ”¯æŒå¤šå¼ ï¼›VGGT/MapAnything æ”¯æŒä»»æ„æ•°é‡ç”šè‡³å•å¼ ã€‚ |
| **å›¾åƒç¼–ç å™¨** | å…±äº« backboneï¼ˆé€šå¸¸æ˜¯é¢„è®­ç»ƒè§†è§‰ transformer æˆ– CNNï¼‰ | ä½¿ç”¨å¦‚ DINOv2ã€MAEã€ViTã€Swinã€ConvNeXt ç­‰ã€‚æå– dense feature mapsã€‚ |
| **ç‰¹å¾åŒ¹é… / èåˆ** | Transformer å…¨å±€æ³¨æ„åŠ› (cross-/self-attention) èåˆä¸åŒè§†è§’ç‰¹å¾ | DUSt3R æ˜¯ pairwise cross-attentionï¼›MUSt3R/MonST3R æ˜¯ multi-view attentionï¼›VGGT/MapAnything æ˜¯ frame/global alternating attentionã€‚ |
| **ä¸‰ç»´ç»“æ„è§£ç ** | ä»èåˆç‰¹å¾é¢„æµ‹å‡ ä½•å±æ€§ï¼ˆæ·±åº¦ã€ç›¸æœºã€ç‚¹äº‘ã€ç›¸å¯¹ä½å§¿ç­‰ï¼‰ | \- é€šè¿‡ MLP/decoder head è¾“å‡º dense depth æˆ– correspondenceï¼›  
\- å†ç”¨å‡ ä½•çº¦æŸæ¢å¤ç›¸æœºã€åœºæ™¯ç‚¹ã€‚ |
| **åœºæ™¯å¯¹é½ä¸èåˆ** | æ˜¾å¼æˆ–éšå¼åœ°å¯¹é½å¤šè§†å›¾ç‚¹äº‘åˆ°ç»Ÿä¸€åæ ‡ç³» | é€šå¸¸é¢„æµ‹ç›¸å¯¹å§¿æ€ / å°ºåº¦ï¼Œç„¶åèåˆå„è§†è§’é‡å»ºã€‚ |
| **è®­ç»ƒç›‘ç£** | ç”¨å·²çŸ¥å‡ ä½•æ ‡æ³¨æˆ–å‡ ä½•ä¸€è‡´æ€§æŸå¤± | æ•°æ®é›†åŒ…æ‹¬ï¼šMegaDepthã€ScanNetã€Hypersimã€Tanks&Templesã€CO3Dã€BlendedMVSã€ImageNet-derivedç­‰ã€‚æŸå¤±åŒ…æ‹¬å‡ ä½•é‡æŠ•å½±ã€æ·±åº¦ä¸€è‡´æ€§ã€åŒ¹é…ä¸€è‡´æ€§ç­‰ã€‚ |
| **è¾“å‡º** | \- æ¯å¼ å›¾çš„æ·±åº¦å›¾  
\- æ¯å¯¹å›¾çš„ç›¸å¯¹ç›¸æœºä½å§¿  
\- å¯é‡å»ºç‚¹äº‘ / ç¨ å¯†ç»“æ„ | ç«¯åˆ°ç«¯æ¨ç†ï¼Œæ— éœ€SfMæˆ–NeRFä¼˜åŒ–ã€‚æ¨ç†é€Ÿåº¦å¿«ã€‚ |

* * *

# âš™ï¸ ä¸‰ã€äº”ä¸ªæ¨¡å‹çš„æ¼”è¿›è„‰ç»œï¼ˆä» DUSt3R åˆ° MapAnythingï¼‰
======================================

| æ¨¡å‹ | æ ¸å¿ƒåˆ›æ–° | æŠ€æœ¯è·¯çº¿ | è¾“å‡º | å±€é™ |
| --- | --- | --- | --- | --- |
| **DUSt3R (CVPR 2024)** | Pairwise Transformer æ¨ç†ä¸¤å¼ å›¾çš„å‡ ä½•å…³ç³» | åŒåˆ†æ”¯ Transformer, cross-attention, è¾“å‡ºç‚¹å¯¹åº” | é¢„æµ‹ dense correspondences + depth + relative pose | åªèƒ½åŒå›¾ï¼Œä¸èƒ½å¤šè§†å›¾ï¼›ä¸èƒ½ç»Ÿä¸€åœºæ™¯ |
| **MUSt3R (ECCV 2024)** | Generalize DUSt3R to multiple images | å¤šè§† transformer, å…±äº«ç¼–ç å™¨ï¼Œglobal latent alignment | å¤šå›¾ joint reconstruction | éš¾è®­ç»ƒï¼›æ˜¾å­˜å¼€é”€å¤§ |
| **MonST3R (ICLR 2025)** | å•ç›®å›¾åƒä¹Ÿå¯ â†’ learn metric scale | å¼•å…¥è‡ªç›‘ç£ scale head, monocular pretraining | å•å¼ å›¾ä¹Ÿè¾“å‡º metric 3D depth | ä½çº¹ç†åœºæ™¯ä¸ç¨³å®šï¼›ä¾èµ–å¤§é‡é¢„è®­ç»ƒ |
| **VGGT (CVPR 2025, Oxford-VGG)** | â€œVisual Geometry Grounded Transformerâ€ç»Ÿä¸€å¤šä»»åŠ¡å‡ ä½•é¢„æµ‹ | Frame-wise + global alternating attention, å¤šä»»åŠ¡å¤´ï¼šç›¸æœº/æ·±åº¦/ç‚¹äº‘/è¿½è¸ª | ç›¸æœºã€æ·±åº¦ã€ç‚¹äº‘ã€ç‚¹è¿½è¸ª | æ²¡æœ‰å…‰ç…§æˆ–æè´¨å»ºæ¨¡ï¼›å‡ ä½•ä¸ºä¸» |
| **MapAnything (CVPR 2025)** | â€œUniversal Feed-forward 3D Reconstructionâ€ â†’ å¯è¾“å…¥ä»»æ„å‡ ä½•å½¢å¼ | Factorized metric reconstruction (depth + local rays + poses) | é€šç”¨å‡ ä½•é‡å»ºè¾“å‡ºï¼Œå¯æ”¯æŒå¤šä»»åŠ¡ | æ¨¡å‹åºå¤§ï¼›è®­ç»ƒæ•°æ®æå¤§ï¼›å¤–è§‚æœªå»ºæ¨¡ |

* * *

# ğŸ§  å››ã€å®ƒä»¬å…±åŒçš„ç‰¹å¾ä¸å“²å­¦ï¼ˆé«˜åº¦æŠ½è±¡å±‚é¢ï¼‰
=======================

| å±‚æ¬¡ | å…±åŒç‰¹å¾ | å«ä¹‰ä¸ä½œç”¨ |
| --- | --- | --- |
| **A. è¡¨å¾å­¦ä¹ èŒƒå¼** | åŸºäº Transformer çš„ Dense Visual Representation | é€šè¿‡å­¦ä¹  dense ç‰¹å¾ç©ºé—´ï¼Œä½¿å‡ ä½•å…³ç³»èƒ½è¢«çº¿æ€§åŒ–æˆ–ç›´æ¥é¢„æµ‹ |
| **B. å‡ ä½•æ˜¾å¼åŒ–ï¼ˆgeometry groundingï¼‰** | åœ¨ç‰¹å¾å­¦ä¹ ä¸­åŠ å…¥å‡ ä½•çº¦æŸï¼ˆç›¸å¯¹ä½å§¿ã€ä¸‰è§’åŒ–ã€æŠ•å½±ä¸€è‡´æ€§ï¼‰ | ç¡®ä¿ç½‘ç»œå­¦åˆ°çš„è¡¨ç¤ºæ˜¯ metrically groundedï¼Œè€Œéä»…è¯­ä¹‰ç‰¹å¾ |
| **C. å‰é¦ˆé‡å»ºï¼ˆFeed-Forward Reconstructionï¼‰** | ä¸ä¾èµ– iterative BA æˆ– NeRF ä¼˜åŒ– | æ¨ç†æ—¶é—´çŸ­ï¼ˆç§’çº§ç”šè‡³æ¯«ç§’çº§ï¼‰ |
| **D. å¤šä»»åŠ¡ä¸€è‡´è¾“å‡º** | åŒæ—¶é¢„æµ‹ depth / camera / 3D points / tracks | å­¦ä¹ ç»Ÿä¸€çš„åœºæ™¯æ½œç©ºé—´ï¼Œå‡å°‘å¤šä»»åŠ¡åˆ†è£‚ |
| **E. æ•°æ®é©±åŠ¨å‡ ä½•å…ˆéªŒ** | å¤§è§„æ¨¡æ··åˆæ•°æ®é›†è®­ç»ƒï¼Œå–ä»£ä¼ ç»Ÿå‡ ä½•çº¦æŸ | ç½‘ç»œé€šè¿‡æ•°æ®å­¦ä¹ â€œä¸–ç•Œçš„å‡ ä½•åˆ†å¸ƒâ€ |
| **F. æ¨¡å‹ç»Ÿä¸€æ€§ä¸å¯æ‰©å±•æ€§** | å¯æ‰©å±•åˆ°ä»»æ„è¾“å…¥æ•°é‡ã€åœºæ™¯ç±»å‹ï¼ˆå®¤å†…ã€æˆ·å¤–ï¼‰ | é€šç”¨æ€§å¼ºï¼Œå¯è¿ç§» |
| **G. NeRF/3DGS äº’è¡¥æ€§** | è¾“å‡ºæ˜¾å¼å‡ ä½•ï¼Œå¯ä½œä¸º NeRF/3DGS åˆå§‹åŒ–æˆ–çº¦æŸ | è¡¥å…¨â€œä¼˜åŒ–å¼éšå¼æ¸²æŸ“â€ä¸â€œå­¦ä¹ å¼æ˜¾å¼å‡ ä½•â€ä¹‹é—´çš„é¸¿æ²Ÿ |

* * *

# ğŸ¨ äº”ã€ä¸ NeRF / 3DGS ç»“åˆçš„é€»è¾‘æ¥å£
==========================

è¿™äº›æ¨¡å‹è™½ç„¶ä¸åšæ¸²æŸ“ï¼Œä½†å¤©ç„¶ä¸º NeRF / 3DGS æä¾›ï¼š

1.  **ç›¸æœºå§¿æ€**ï¼ˆç”¨äº NeRF åˆå§‹åŒ–ï¼‰
2.  **æ·±åº¦ / ç‚¹äº‘**ï¼ˆç”¨äº 3DGS åˆå§‹åŒ– Gaussian åˆ†å¸ƒï¼‰
3.  **ç‰¹å¾è¿½è¸ª**ï¼ˆæ”¯æŒåŠ¨æ€åœºæ™¯æˆ–æ—¶é—´ä¸€è‡´æ€§ï¼‰
4.  **è¯­ä¹‰è¡¨å¾**ï¼ˆç”¨äºå¯ç¼–è¾‘ NeRFï¼‰

å› æ­¤ï¼Œåœ¨æ–°çš„ hybrid ç³»ç»Ÿä¸­ï¼Œé€šå¸¸é‡‡ç”¨ï¼š

```
å›¾åƒè¾“å…¥ â†’ (VGGT / MapAnything / MonST3R) ç”Ÿæˆå‡ ä½•åˆå§‹åŒ–
       â†’ (NeRF / 3DGS) åŸºäºå‡ ä½•ç»†åŒ–æ¸²æŸ“ä¸å¤–è§‚å»ºæ¨¡
```

* * *

# ğŸ§¾ å…­ã€ç»Ÿä¸€çš„æ¨¡å‹è®¾è®¡æ¨¡æ¿ï¼ˆå‡ ä¹æ‰€æœ‰æ¨¡å‹å…±æœ‰ï¼‰
========================

```text
Input: {I_1, I_2, â€¦, I_N}
â†“
Shared Image Encoder (ViT/DINO/ResNet)
â†“
Feature Tokens {f_1, f_2, â€¦, f_N}
â†“
Multi-View Transformer
    â”œâ”€ Frame-wise attention
    â”œâ”€ Cross-view attention
    â””â”€ Global latent fusion
â†“
Geometric Heads:
    â”œâ”€ Depth prediction
    â”œâ”€ Camera pose estimation
    â”œâ”€ Point cloud / correspondence prediction
    â”œâ”€ (Optional) Metric scale head (MonST3R)
â†“
Scene Unification (alignment, fusion)
â†“
Outputs: Depth maps, 3D points, poses, tracks
```

* * *

# âš–ï¸ ä¸ƒã€å…±åŒä¼˜ç‚¹ä¸å±€é™æ€»ç»“
==============

| ç»´åº¦ | ä¼˜ç‚¹ | å±€é™ |
| --- | --- | --- |
| **é€Ÿåº¦** | å‰é¦ˆæ¨ç†ï¼ˆfeed-forwardï¼‰å‡ ä¹å®æ—¶ï¼›æ¯” NeRF/BA å¿«ä¸¤ä¸ªæ•°é‡çº§ | å¯èƒ½ç‰ºç‰²ç»†èŠ‚ç²¾åº¦ |
| **é€šç”¨æ€§** | æ”¯æŒå¤šä»»åŠ¡ã€å¤šè¾“å…¥å½¢å¼ã€ä¸åŒåœºæ™¯ | æ— æ³•å¤„ç†åŠ¨æ€ã€åå°„ã€é€æ˜ã€å…‰ç…§å¤æ‚åœºæ™¯ |
| **å‡ ä½•ä¸€è‡´æ€§** | è·¨è§†å›¾å‡ ä½•ä¸€è‡´æ€§å¼º | å¤§è§„æ¨¡è®­ç»ƒéš¾ä»¥å®Œç¾ä¿æŒå…¨å±€å°ºåº¦ä¸€è‡´æ€§ |
| **æ•°æ®éœ€æ±‚** | å¯æ³›åŒ–åˆ°ä¸åŒæ•°æ®é›† | è®­ç»ƒæ•°æ®é‡å·¨å¤§ï¼ˆTBçº§ï¼‰ |
| **å¤–è§‚é‡å»ºèƒ½åŠ›** | ä¼˜äºçº¯æ·±åº¦å­¦ä¹ æ·±åº¦ä¼°è®¡ | ä¸åŠ NeRF/3DGS çš„æ¸²æŸ“ç»†èŠ‚ä¸å…‰ç…§ç²¾åº¦ |

* * *



ğŸ§  DUSt3R æŠ€æœ¯å®ç°å…¨è§£æï¼ˆå®ç°è·¯å¾„ / æ¨¡å—ç»“æ„ / æ•°å­¦æ¨ç†ï¼‰
=====================================

* * *

ğŸ§­ ä¸€ã€æ¨¡å‹æ€»ä½“ä»»åŠ¡å®šä¹‰
-------------

> **è¾“å…¥**ï¼šä¸¤å¼ æœªé…å‡†çš„ RGB å›¾åƒ  $I_A, I_B$ ï¼ˆå·²çŸ¥å†…å‚  $K_A, K_B$ ï¼‰  
> **è¾“å‡º**ï¼š
> 
> *   ç›¸å¯¹ç›¸æœºå§¿æ€  $T_{A \to B} = [R|t]$ 
> *   æ¯å¼ å›¾å¯¹åº”çš„**ç¨ å¯†3Dç‚¹å›¾ï¼ˆpoint mapï¼‰**
> *   æ·±åº¦å›¾  $D_A, D_B$ 
>     

å®ƒä¸è¿›è¡Œä¼ ç»Ÿ SfM/MVS çš„æ˜¾å¼åŒ¹é…å’ŒBAä¼˜åŒ–ï¼Œè€Œæ˜¯ï¼š

âœ… **ä¸€æ¬¡å‰å‘æ¨ç†ï¼ˆFeed-forwardï¼‰**  
âœ… **ç«¯åˆ°ç«¯ Transformer**  
âœ… **è‡ªåŠ¨å­¦ä¼šåŒ¹é… + æ·±åº¦ + ä½å§¿**

* * *

ğŸ§© äºŒã€æ¨¡å‹ç»“æ„å…¨æ‹†è§£
------------

DUSt3R çš„æ¶æ„å¯ä»¥åˆ†ä¸ºä»¥ä¸‹ç»„ä»¶ï¼š

| æ¨¡å— | ç»“æ„ç±»å‹ | è¾“å…¥ | è¾“å‡º | ä½œç”¨ |
| --- | --- | --- | --- | --- |
| **Image Encoder** | CNN æˆ– ViTï¼ˆDINOv2ï¼‰ | å›¾åƒ  $I_A, I_B$  | Dense feature maps  $F_A, F_B$  | å­¦ä¹ ä½å±‚çº¹ç†+é«˜å±‚è¯­ä¹‰ |
| **Cross-Attention Transformer** | å¤šå±‚ Attention å— |  $(F_A, F_B)$  | èåˆåçš„ pairwise feature | å­¦ä¹ å›¾åƒä¹‹é—´çš„å¯¹åº”å…³ç³» |
| **Point Map Head** | MLP + Conv | åŠ å¼ºèåˆåçš„ç‰¹å¾ | æ¯åƒç´  3D åæ ‡  $P_A(x,y)$  | é¢„æµ‹æ¯ä¸ªåƒç´ çš„ä¸‰ç»´ä½ç½® |
| **Relative Pose Head** | Global Feature â†’ MLP | é«˜ç»´ latent | ç›¸å¯¹å§¿æ€ (R,t) | æ¨ç†ç›¸æœºä¹‹é—´çš„ç›¸å¯¹ä½å§¿ |
| **Consistency Alignment Layer** | Geometry Projection |  $(P_A, T_{A\to B})$  | é‡æŠ•å½±åçš„  $P'_B$  | ä¿è¯å‡ ä½•ä¸€è‡´æ€§ |

* * *

âš™ï¸ ä¸‰ã€æ ¸å¿ƒæœºåˆ¶è¯¦è§£ï¼ˆHow 2D â†’ 3Dï¼‰
------------------------

### 3.1 ç‰¹å¾æå–é˜¶æ®µ

æ¯å¼ å›¾åƒé€šè¿‡ç¼–ç å™¨å¾—åˆ° token-like ç‰¹å¾ï¼š

$$
F_A \in \mathbb{R}^{H \times W \times C}, \quad F_B \in \mathbb{R}^{H \times W \times C}
$$

* * *

### 3.2 Cross-Attentionï¼šéšå¼åŒ¹é…å­¦ä¹ 

Transformer ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›ï¼ˆCross-Attentionï¼‰ï¼š

$$
\text{Attn}(F_A, F_B) = \text{softmax}(F_A W_Q (F_B W_K)^T / \sqrt{d}) (F_B W_V)
$$

è¿™ç›¸å½“äº**æ¯ä¸ªåƒç´ å»å›¾Bä¸­å¯»æ‰¾è‡ªå·±æœ€å¯èƒ½çš„å¯¹åº”ç‚¹**ï¼Œä½†æ— éœ€æ˜¾å¼åŒ¹é…çŸ©é˜µã€‚

* * *

### 3.3 Point Map Headï¼šç›´æ¥é¢„æµ‹ 3D ç‚¹åæ ‡

å¯¹äºæ¯ä¸ª A çš„åƒç´   $(x,y)$ ï¼ŒTransformer è¾“å‡ºä¸€ä¸ª 3D å‘é‡ï¼š

$$
\hat{P}_A(x,y) \in \mathbb{R}^3
$$

è¿™è¡¨ç¤ºå®ƒåœ¨ç›¸æœº A åæ ‡ç³»ä¸­çš„ä¼°è®¡ç‰©ç†ä½ç½®ã€‚

æ³¨æ„ï¼š**æ¨¡å‹ä¸æ˜¯å…ˆé¢„æµ‹æ·±åº¦å†åæŠ•å½±ï¼Œè€Œæ˜¯ç›´æ¥å›å½’ç©ºé—´åæ ‡**ï¼

ï¼ˆè™½ç„¶æœ€ç»ˆæ·±åº¦å¯ç”±  $Z$  åˆ†é‡å¾—åˆ°ï¼‰

* * *

### 3.4 Pose Headï¼šå…¨å±€åŒ¹é…å¾—åˆ°ç›¸å¯¹ä½å§¿  $T_{A\to B}$ 

DUSt3R é€šè¿‡**feature pooling + MLP**å¾—åˆ°å§¿æ€ä¼°è®¡ï¼š

$$
[R|t]_{A\to B} = f_\theta(\text{global\_pool}(F_A, F_B))
$$

è®­ç»ƒæ—¶é€šè¿‡**å¯¹é½ç‚¹äº‘**çº¦æŸå®ƒæ­£ç¡®ï¼š

$$
\hat{P}_A \xrightarrow[]{T_{A\to B}} \hat{P}'_A \stackrel{!}{\approx} \hat{P}_B
$$

* * *

### 3.5 å‡ ä½•ä¸€è‡´æ€§æŸå¤±ï¼ˆæ ¸å¿ƒæ•°å­¦çº¦æŸï¼‰

æœ€ä¸»è¦çš„ loss æ˜¯ï¼š

$$
L_{geo} = \|\hat{P}_B - (R_{A\to B} \cdot \hat{P}_A + t_{A\to B})\|_1
$$

å³**æŠŠ A çš„ç‚¹å˜æ¢åˆ° Bï¼Œçœ‹æ˜¯å¦å¯¹å¾—ä¸Š B çš„ç‚¹äº‘**ã€‚

âš ï¸ è®­ç»ƒæ˜¯**åŒå‘**çš„ï¼ˆå³ Aâ†’B + Bâ†’Aï¼‰ï¼Œä¿è¯å¯¹ç§°ä¸€è‡´æ€§ã€‚

* * *

ğŸ”¬ å››ã€æŸå¤±å‡½æ•°ç»„åˆ
-----------

| Loss | å…¬å¼ | ä½œç”¨ |
| --- | --- | --- |
| **Point Reconstruction Loss** |  $\|\hat{P}_X - P^{gt}_X\|$  | ç›‘ç£3Dé¢„æµ‹ |
| **Pose Consistency Loss** |  $\|\hat{T}_{A\to B} - T^{gt}_{A\to B}\|$  | å§¿æ€ç›‘ç£ |
| **Bidirectional Alignment Loss** |  $\|P'_A - P_B\| + \|P'_B - P_A\|$  | å‡ ä½•å¯¹é½ |

æœ€ç»ˆï¼š

$$
L = \lambda_1 L_{point} + \lambda_2 L_{pose} + \lambda_3 L_{align}
$$

* * *

ğŸ äº”ã€æ¨ç†é˜¶æ®µæµç¨‹ï¼ˆInference Pipelineï¼‰
-------------------------------

```
Input: Image A, Image B

F_A, F_B = Encoder(I_A), Encoder(I_B)
F_A', F_B' = CrossAttention(F_A, F_B)

P_A = PointHead(F_A')
P_B = PointHead(F_B')

T_AB = PoseHead(F_A', F_B')

# (å¯é€‰) ç‚¹äº‘å¯¹é½æ˜¾ç¤ºæˆ–å¯¼å‡º
PointCloud = fuse(P_A, T_AB(P_B))
```

* * *

âœ… å…­ã€ä¼˜åŠ¿æ€»ç»“
--------

| ä¼˜åŠ¿ | DUSt3R çš„è¡¨ç° |
| --- | --- |
| å‡ ä½•é¢„æµ‹ç²¾åº¦ | ä¸ç»å…¸SfM/MVSç›¸å½“ç”šè‡³æ›´å¥½ |
| æ¨ç†é€Ÿåº¦ | <1ç§’ï¼Œè¿œå¿«äºNeRF/SfMä¼˜åŒ– |
| å¯ç«¯åˆ°ç«¯è®­ç»ƒ | æ— éœ€æ‰‹å†™å‡ ä½•ç®¡çº¿ |
| å¯æ‰©å±•æ€§ | å·²æ‰©å±•ä¸º MUSt3Rï¼ˆå¤šå›¾ï¼‰/MonST3Rï¼ˆåŠ å°ºåº¦ï¼‰/VGGTï¼ˆé€šç”¨ï¼‰ |

* * *

âš ï¸ ä¸ƒã€å±€é™æ€§ä¸æ”¹è¿›æ–¹å‘
-------------

| å±€é™ | é—®é¢˜ | åç»­æ”¹è¿› |
| --- | --- | --- |
| ä»…åŒå›¾è¾“å…¥ | ä¸èƒ½ç»Ÿä¸€å…¨å±€åœºæ™¯ | MUSt3R æ‰©å±•å¤šè§†å›¾ |
| æ— ç»å¯¹å°ºåº¦ | è¾“å‡ºåªæœ‰ç›¸å¯¹å‡ ä½• | MonST3R åŠ  Scale Head |
| ä¸æ¸²æŸ“å¤–è§‚ | æ— æè´¨/å…‰ç…§ä¿¡æ¯ | å¯ä¸ 3DGS/NeRF ç»“åˆ |
| å¯¹æç«¯ä½çº¹ç†æ•æ„Ÿ | ç¼ºä¹å…ˆéªŒ | å¯åŠ å…¥è¯­ä¹‰æˆ–edge prior |

* * *



## Prompt:
è¯¦è§£dust3r\monst3r\vggt\mapanythingç­‰è¿™äº›æŠ€æœ¯æ¶‰åŠåˆ°çš„æŠ€æœ¯ï¼Œä»¥åŠå…¶æŠ€æœ¯åŸç†ï¼Œç”¨åˆ°äº†å“ªäº›è¯¦ç»†çš„æŠ€æœ¯ä¸€æ­¥ä¸€æ­¥å®ç°ç°æˆæ•´ä¸ªæ¡†æ¶å®ç°

## Response:
å·²æ€è€ƒ 32s

ä¸‹é¢ç»™å‡ºä¸€ä»½**æä¸ºè¯¦å°½ä¸”å¯å¤ç°çš„æŠ€æœ¯è®²è§£**ï¼ŒæŠŠ DUSt3R / MonST3R / VGGT / MapAnything è¿™ä¸€ç±»â€œå›¾åƒâ†’è¡¨å¾â†’å‡ ä½•â€ï¼ˆfeed-forward geometry / 3Rï¼‰æ¨¡å‹ä¸­ç”¨åˆ°çš„æ‰€æœ‰å…³é”®æŠ€æœ¯ã€æ•°å­¦åŸç†ã€æ¨¡å—å®ç°ã€è®­ç»ƒç»†èŠ‚ä¸å·¥ç¨‹è¦ç‚¹**é€æ­¥ä¸²è”æˆä¸€ä¸ªå®Œæ•´å®ç°æ¡†æ¶**ã€‚æˆ‘ä¼šï¼š

*   å…ˆæŠŠè¿™äº›æ–¹æ³•å…±äº«çš„æ ¸å¿ƒæ€æƒ³ä¸ç»„æˆæ¨¡å—é€ä¸€åˆ—æ¸…æ¥šï¼›
*   ç„¶åæŠŠæ¯ä¸ªæ¨¡å—çš„\*\*æ•°å­¦å…¬å¼ / ç®—æ³•æ­¥éª¤ / å…·ä½“å®ç°å»ºè®®ï¼ˆPyTorch é£æ ¼ä¼ªç æ€è·¯ï¼‰\*\*ç»™å‡ºï¼›
*   å†é’ˆå¯¹æ¯ä¸ªæ–¹æ³•çš„**ç‰¹æ®Šæ‰©å±•**ï¼ˆDUSt3R çš„ pointmapã€MonST3R çš„æ—¶åº/è¿åŠ¨å¤„ç†ã€VGGT çš„å¤šè§†/å¤šä»»åŠ¡ä¸é¢„è®­ç»ƒã€MapAnything çš„ factored metric è¾“å‡ºï¼‰ç»™å‡ºå·®å¼‚åŒ–å®ç°è¦ç‚¹ä¸è½åœ°æŠ€å·§ï¼›
*   æœ€åç»™å‡ºè®­ç»ƒæ•°æ®ã€æŸå¤±å‡½æ•°è®¾è®¡ã€è¶…å‚å»ºè®®ã€è°ƒå‚ä¸ debug checklistï¼Œä»¥åŠå¸¸è§å·¥ç¨‹é™·é˜±ä¸è§£å†³ç­–ç•¥ã€‚

æˆ‘ä¼šåœ¨å…³é”®å¤„å¼•ç”¨è®ºæ–‡ / é¡¹ç›®é¡µé¢ä»¥ä¾¿æ ¸éªŒä¸æ·±å…¥é˜…è¯»ï¼ˆDUSt3R, MonST3R, VGGT, MapAnythingï¼‰ã€‚[map-anything.github.io+3CVFå¼€æ”¾è·å–+3arXiv+3](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_DUSt3R_Geometric_3D_Vision_Made_Easy_CVPR_2024_paper.pdf?utm_source=chatgpt.com)

* * *

1 â€” é«˜é˜¶æ‘˜è¦ï¼ˆå…±åŒèŒƒå¼ï¼Œä¸€å¥è¯ç‰ˆï¼‰
===================

è¿™ç±»å·¥ä½œæŠŠä¼ ç»Ÿçš„â€œæµ‹é‡â†’åŒ¹é…â†’ä¸‰è§’åŒ–â†’BAâ€å‡ ä½•ç®¡çº¿æ›¿æ¢ä¸ºâ€œè§†è§‰è¡¨å¾ï¼ˆencoderï¼‰â†’è·¨è§†äº¤äº’ï¼ˆtransformer/attentionï¼‰â†’å‡ ä½•è§£ç å™¨ï¼ˆpointmap / depth / pose headsï¼‰â†’å…¨å±€å¯¹é½/ä¸€è‡´æ€§æŸå¤±â€çš„**ç«¯åˆ°ç«¯å‰é¦ˆé‡å»º**ã€‚å®ƒä»¬å…±äº«åŒä¸€å¥—æ„ä»¶ï¼šå¼ºå¤§çš„ dense encoderï¼ˆViT/CNNï¼‰ã€cross-view attention / correlation volumeã€å‡ ä½•å›å½’å¤´ï¼ˆdepth/pointmap/poseï¼‰ã€å‡ ä½•ä¸€è‡´æ€§æŸå¤±ï¼ˆé‡æŠ•å½±/ç‚¹äº‘å¯¹é½/åŒå‘çº¦æŸï¼‰ä¸å¤šé˜¶æ®µè®­ç»ƒï¼ˆåˆæˆ+çœŸå®ï¼‰ç®¡çº¿ã€‚[arXiv+1](https://arxiv.org/abs/2312.14132?utm_source=chatgpt.com)

* * *

2 â€” å…±ç”¨æ¨¡å—ä¸å®ç°ç»†èŠ‚ï¼ˆé€æ¨¡å—æ‹†è§£ï¼‰
====================

ä¸‹é¢æŠŠå®ç°æ‰€éœ€çš„æ¯ä¸ªæ¨¡å—éƒ½æ‹†æˆï¼š**ç›®æ ‡ â†’ æ•°å­¦ / ç®—æ³• â†’ å®ç°ç»†èŠ‚ï¼ˆä¼ªç  & å·¥ç¨‹æç¤ºï¼‰**ã€‚

* * *

2.1 è¾“å…¥ä¸é¢„å¤„ç†ï¼ˆå¿…éœ€ï¼‰
--------------

**ç›®æ ‡**ï¼šæŠŠåŸå§‹ RGB å›¾åƒè½¬æˆç½‘ç»œèƒ½å¤„ç†çš„ tensorï¼ˆå¤šå°ºåº¦ tokenï¼‰ã€‚

**å®ç°è¦ç‚¹**ï¼š

*   æ ‡å‡†åŒ–ï¼ˆmean/stdï¼‰ï¼Œå¯é€‰ gamma æ ¡æ­£ï¼ˆåœ¨æ•°æ®é›†ä¸ä¸€è‡´æ—¶ï¼‰ï¼›
*   å°ºåº¦ï¼šè®­ç»ƒæ—¶é€šå¸¸é‡‡ `HÃ—W âˆˆ {256Ã—320, 384Ã—512}` ç­‰ï¼Œæ¨ç†å¯å…¨åˆ†è¾¨ç‡ï¼›
*   é‡‡ç”¨éšæœº crop + color jitter + blur å¢å¼ºï¼ˆå¯¹å¤šè§†å‡ ä½•è¦è°¨æ…ï¼Œä¸ç ´åå‡ ä½•ä¸€è‡´æ€§ï¼‰ï¼›
*   è‹¥æ— å†…å‚ï¼Œåˆ™å¯è®­ç»ƒæˆ–é¢„æµ‹å†…å‚ï¼ˆDUSt3R æ”¯æŒä¸å«å†…å‚çš„æƒ…å†µï¼‰ï¼›è‹¥æœ‰å†…å‚ï¼ŒæŠŠ K ä½œä¸ºè¾“å…¥ token æˆ– concatenate åˆ° featureã€‚[arXiv](https://arxiv.org/html/2312.14132v2?utm_source=chatgpt.com)

**ä¼ªç **:

```python
# PyTorch-like
I = load_image(path)
I = center_crop_or_resize(I, size=(H,W))
I = (I - mean)/std
if train:
    I = random_color_jitter(I)
```

* * *

2.2 Dense Image Encoderï¼ˆViT / CNN backboneï¼‰
-------------------------------------------

**ç›®æ ‡**ï¼šæŠŠæ¯å¼ å›¾åƒç¼–ç ä¸º dense feature map  $F \in \mathbb{R}^{H' \times W' \times C}$ ã€‚

**å¸¸è§é€‰æ‹©**ï¼š

*   ViT / Swin Transformer / DINOv2 ç‰¹å¾ï¼ˆå¯åˆ©ç”¨è‡ªç›‘ç£é¢„è®­ç»ƒï¼‰ï¼›
*   æˆ–è€… Conv encoderï¼ˆResNetã€ConvNeXtï¼‰åŠ ä¸Š FPN åš multi-scaleã€‚

**å®ç°ç»†èŠ‚**ï¼š

*   æ¨èä½¿ç”¨ pretrained ViT/DINOv2 å¹¶ fine-tuneï¼ˆæé«˜å‡ ä½•æ„ŸçŸ¥ï¼‰ï¼›VGGT æ˜ç¡®ä½¿ç”¨å¼º pretrained backbone æ¥æé«˜æ³›åŒ–ã€‚[arXiv](https://arxiv.org/html/2503.11651v1?utm_source=chatgpt.com)
*   è¾“å‡ºä¿ç•™ç©ºé—´ç»“æ„ï¼ˆä¸è¦ global poolingï¼‰ï¼Œåç»­ attention éœ€è¦ per-pixel tokenã€‚

**ä¼ªç **:

```python
# F: [B, C, H', W']
F = backbone(I)  # e.g., ViT features reshaped to H'Ã—W' tokens
```

* * *

2.3 Cross-View Interactionï¼ˆæ ¸å¿ƒï¼šattention / correlationï¼‰
------------------------------------------------------

**ç›®æ ‡**ï¼šè®©ä¸åŒè§†å›¾çš„åƒç´  / token å½¼æ­¤â€œçœ‹åˆ°â€å¯¹æ–¹ï¼Œä»è€Œéšå¼å»ºç«‹å¯¹åº”å…³ç³»ï¼ˆç­‰ä»·äº learnable matching / correlation volumeï¼‰ã€‚

**ä¸¤ç±»å®ç°**ï¼š

1.  **Correlation volume**ï¼ˆæ˜¾å¼ï¼‰
    *   å°† F\_A ä¸ F\_B çš„ patch features åšå†…ç§¯ç”Ÿæˆ 4D correlation ï¼ˆä»£ä»·é«˜ï¼‰ã€‚
2.  **Cross-attention Transformer**ï¼ˆéšå¼åŒ¹é…ï¼Œå¸¸ç”¨ï¼‰
    *   æŠŠæ¯åƒç´ è§†ä¸º tokenï¼Œæ‰§è¡Œ multi-head cross-attentionï¼šQ=F\_A Wq, K=F\_B Wk, V=F\_B Wvã€‚
    *   attention map çš„ softmax è¡Œä¸ºéšå¼è¡¨ç¤ºâ€œåŒ¹é…æ¦‚ç‡â€ã€‚

**æ•°å­¦**ï¼š

$$
\text{Attn}(Q,K,V) = \text{softmax}\!\left( \frac{QK^\top}{\sqrt{d}} \right) V
$$

**å®ç°ç»†èŠ‚**ï¼š

*   å¯¹å¤§åˆ†è¾¨ç‡ä½¿ç”¨ windowed / hierarchical attentionï¼ˆSwin styleï¼‰ä»¥é™ä½å¤æ‚åº¦ï¼›VGGT ä½¿ç”¨ frame-wise + global alternating attention ä»¥æ”¯æŒå¤§é‡è§†å›¾ã€‚[arXiv](https://arxiv.org/html/2503.11651v1?utm_source=chatgpt.com)
*   ä¸ºäº†ç¨³å®šï¼Œå¯åœ¨ attention map ä¸ŠåŠ å¯å­¦ä¹ æ¸©åº¦ã€maskï¼ˆé®æŒ¡æŠ‘åˆ¶ï¼‰æˆ– confidence weightingã€‚

**ä¼ªç **:

```python
Q = F_A @ Wq
K = F_B @ Wk
V = F_B @ Wv
attn = softmax(Q @ K.transpose(-2,-1) / sqrt(d))
F_A_fused = attn @ V  # cross-influence from B to A
```

* * *

2.4 Geometry Decoder / Pointmap Headï¼ˆå…³é”®ï¼‰
----------------------------------------

**ç›®æ ‡**ï¼šä»èåˆåçš„ç‰¹å¾ç›´æ¥å›å½’ 3D è¡¨ç¤ºï¼ˆpointmapã€depthã€æˆ– XYZ ç›´æ¥å›å½’ï¼‰ã€‚

### ä¸¤ä¸ªä¸»æµè¡¨ç¤º

*   **Pointmapï¼ˆDUSt3Rï¼‰**ï¼šå¯¹æ¯ä¸ªåƒç´ å›å½’ä¸€ä¸ªä¸‰ç»´ç‚¹åæ ‡  $\hat{P}(u,v) \in \mathbb{R}^3$ ï¼ˆåœ¨æŸä¸ªå‚ç…§ç³»ä¸‹ï¼‰ï¼Œä¼˜åŠ¿æ˜¯ç›´æ¥ã€ç¨ å¯†ã€èƒ½è‡ªç„¶å¯¼å‡ºå¯¹åº”å…³ç³»ä¸æ·±åº¦ã€‚[arXiv](https://arxiv.org/abs/2312.14132?utm_source=chatgpt.com)
*   **Depth map**ï¼šå›å½’æ ‡é‡æ·±åº¦  $D(u,v)$ ï¼Œå†ç»“åˆ K åæŠ•å½±å¾—åˆ° 3D ç‚¹ï¼›å¸¸è§äºå•ç›®æ·±åº¦æ–¹æ³•ä¸ MVSã€‚
*   **Hybrid**ï¼šåŒæ—¶è¾“å‡º depth + residual XYZ ç”¨äºé«˜ç²¾åº¦å±€éƒ¨ä¿®æ­£ã€‚

**å®ç°ç»†èŠ‚**ï¼š

*   ç”¨ Conv/MLP Headsï¼š`Conv( C -> 128 ) -> ReLU -> Conv(128 -> 3)` è¾“å‡ºä¸‰ä¸ªé€šé“ï¼ˆX,Y,Zï¼‰æˆ–æ·»åŠ  sigmoid/exp ä»¥ä¿è¯ Z>0ï¼›
*   è‹¥å›å½’ absolute XYZï¼Œéœ€ç»™å®šæˆ–ä¼°è®¡å°ºåº¦ / global frameï¼›è‹¥æ²¡æœ‰ï¼Œåˆ™å›å½’åˆ°å±€éƒ¨ç›¸æœºåæ ‡ï¼ˆéœ€è¦å†åšå¯¹é½ï¼‰ã€‚DUSt3R å›å½’å±€éƒ¨ canonical frameï¼Œä¹‹åå†å¯¹é½åˆ° globalã€‚[arXiv](https://arxiv.org/abs/2312.14132?utm_source=chatgpt.com)

**ä¼ªç **:

```python
# F_fused: [B, C, H', W']
P = ConvBlock(F_fused)  # -> [B, 3, H', W']
# Optionally: D = P[...,2]
```

* * *

2.5 Pose Headï¼ˆç›¸æœºå¤–å‚ä¼°è®¡ï¼‰
---------------------

**ç›®æ ‡**ï¼šä»ä¸¤ä¸ªå›¾åƒçš„è”åˆç‰¹å¾ä¸­å›å½’ç›¸å¯¹ä½å§¿  $R, t$ ã€‚

**å®ç°æ–¹å¼**ï¼š

*   åšå…¨å±€ poolingï¼ˆavg æˆ– attention poolingï¼‰å¾—åˆ° global descriptor  $g$ ï¼›
*   MLP( $g$ ) è¾“å‡º  $\hat{R}$ ï¼ˆå‚æ•°åŒ–ä¸º axis-angle / quaternionï¼‰ä¸  $\hat{t}$ ï¼›
*   è‹¥éœ€è¦å†…å‚é¢„æµ‹ï¼Œå¯åœ¨åŒä¸€ head è¾“å‡º fx, fy, cx, cyï¼ˆæˆ–ç”¨ fixed Kï¼‰ã€‚DUSt3R å¯ä»¥åœ¨æœªçŸ¥å†…å‚çš„æ¡ä»¶ä¸‹å›å½’ pointmapï¼ˆä»è€Œæ— éœ€é¢„çŸ¥ Kï¼‰ã€‚[arXiv](https://arxiv.org/abs/2312.14132?utm_source=chatgpt.com)

**æŸå¤±**ï¼š

*   æ—‹è½¬ï¼šä½¿ç”¨ quaternion æˆ– so(3) æŸå¤±ï¼ˆä¾‹å¦‚ geodesic distanceï¼‰ï¼Œå¹³ç§»ï¼šL2 æŸå¤±ï¼ˆæˆ–å¯¹å°ºåº¦ä¸æ•æ„Ÿçš„æ–¹å‘æŸå¤±ï¼‰ã€‚

**ä¼ªç **:

```python
g = attention_pool(F_A_fused, F_B_fused)
pose = MLP(g)  # outputs 6D pose param
R, t = pose2RT(pose)
```

* * *

2.6 å¯¹é½ / å…¨å±€é…å‡†ï¼ˆå¤šè§†èåˆï¼‰
-------------------

**ç›®æ ‡**ï¼šå¦‚æœæœ‰å¤šå¯¹ pairwise pointmapsï¼ŒæŠŠå®ƒä»¬å¯¹é½åˆ°ä¸€ä¸ªå…¨å±€åæ ‡ç³»ï¼ˆMap fusionï¼‰ã€‚

**å¸¸è§æ–¹æ¡ˆ**ï¼š

*   **Weighted ICP-like alignment**ï¼šç”¨ pairwise 3D correspondences + confidence weights æ‰§è¡ŒåŠ æƒåˆšæ€§å¯¹é½ï¼ˆé—­å¼è§£æˆ–è¿­ä»£æœ€å°äºŒä¹˜ï¼‰ã€‚
*   **é²æ£’ä¼˜åŒ–**ï¼šç”¨ RANSAC è¿‡æ»¤é”™è¯¯åŒ¹é…ï¼Œç„¶å solve Umeyama / Horn ç®—æ³•å¾—åˆ°  $s,R,t$ ã€‚DUSt3R æå‡ºç®€å•æœ‰æ•ˆçš„ global alignment æŠŠ pairwise pointmaps æ”¾åˆ°å…±åŒå‚è€ƒç³»ã€‚[arXiv](https://arxiv.org/abs/2312.14132?utm_source=chatgpt.com)

**å®ç°ä¼ªç **:

```python
# Given many P_i (per-image pointmaps) and pairwise transforms T_ij
# Solve global poses G_i minimizing sum || G_j^{-1} G_i P_i - P_j || weighted
# Use robust solver or iterative reweighting
```

* * *

2.7 æŸå¤±å‡½æ•°ä¸ç›‘ç£ç­–ç•¥ï¼ˆè®­ç»ƒæ ¸å¿ƒï¼‰
-------------------

**å¸¸è§ loss ç»„åˆ**ï¼ˆå¯ç›´æ¥å¥—ç”¨ï¼‰ï¼š

1.  **Point regression loss** (L1 / L2 with GT points if available)ï¼š  
     $L_{point} = \| \hat P - P_{gt} \|_1$ 
2.  **Depth loss**ï¼ˆå¦‚æœç”¨ depth è¡¨ç¤ºï¼‰ï¼š  
     $L_{depth} = \| \hat D - D_{gt} \|_1$  æˆ– scale-invariant lossã€‚
3.  **Pose loss**ï¼ˆæ—‹è½¬ + å¹³ç§»ï¼‰:  
     $L_{pose}= \lambda_{rot} \cdot d_{rot}(R,\hat R) + \lambda_{trans}\|t-\hat t\|$ .
4.  **Geometric consistency / alignment lossï¼ˆå…³é”®ï¼‰**ï¼šåŒå‘æŠŠ A ç‚¹å˜æ¢åˆ° B å¯¹é½ B ç‚¹ï¼š  
     $L_{align} = \| P_B - (R_{AB} P_A + t_{AB}) \|_1$ ï¼ˆå¹¶åš AB + BAï¼‰ã€‚
5.  **Confidence weighting**ï¼šè¾“å‡ºæ¯ä¸ªç‚¹ä¸€ä¸ªç½®ä¿¡åº¦  $c(u,v)$ ï¼Œç”¨  $c$  åŠ æƒç‚¹æŸå¤±å¹¶é¢å¤–è®­ç»ƒç½®ä¿¡åº¦ï¼ˆbinaryäº¤å‰æˆ–å›å½’çœŸå®è¯¯å·®ï¼‰ã€‚[arXiv](https://arxiv.org/abs/2312.14132?utm_source=chatgpt.com)
6.  **Smoothness / Normal consistency**ï¼šå¯¹æ·±åº¦æˆ–ç‚¹äº‘æ³•å‘åšå¹³æ»‘çº¦æŸï¼Œé¿å…å™ªå£°ã€‚
7.  **Self-supervision / photometric loss**ï¼ˆåœ¨æ²¡æœ‰çœŸæ·±åº¦æ—¶ï¼‰ï¼šåŸºäºé‡æŠ•å½±åˆæˆ  $ \hat I_B = \Pi(T_{A\to B}, \hat D_A, I_A)$  çš„åƒç´ é‡æ„å·®ã€‚å¸¸ç”¨äº MonST3R çš„æ— ç›‘ç£æˆ–å¼±ç›‘ç£é˜¶æ®µã€‚[arXiv](https://arxiv.org/html/2410.03825v2?utm_source=chatgpt.com)

* * *

2.8 Confidence / Occlusion Handling
-----------------------------------

**ç›®æ ‡**ï¼šé®æŒ¡å’Œè§†å·®æ­§ä¹‰ä¼šå¸¦æ¥é”™è¯¯ç‚¹ï¼Œæ¨¡å‹é€šå¸¸è¾“å‡º per-pixel confidence æˆ–å¯é®æŒ¡æ©ç ã€‚

**å®ç°**ï¼š

*   åœ¨ decoder åŒæ—¶è¾“å‡º `logit_conf` -> `sigmoid`ï¼Œåœ¨æŸå¤±ä¸­ä»¥ `conf * point_loss + (1-conf) * penalty` å½¢å¼è®­ç»ƒï¼›
*   ä½¿ç”¨ cross-attention ä¸­çš„ attention magnitude ä½œä¸ºå¯¹åº”ç½®ä¿¡çš„ proxyã€‚

* * *

3 â€” ä»æ¨¡å—åˆ°å®Œæ•´æµæ°´çº¿ï¼ˆä¸€æ­¥ä¸€æ­¥çš„å®ç°è¯´æ˜ï¼‰
========================

ä¸‹é¢ç»™å‡ºä¸€ä¸ªå®Œæ•´è®­ç»ƒ/æ¨ç†æµæ°´ï¼ˆåŒæ—¶åŒ…å«å¿…è¦çš„å®ç°å»ºè®®ï¼‰ï¼Œå¯ç›´æ¥ä½œä¸ºå¤ç°è·¯çº¿ã€‚

* * *

3.1 æ•°æ®å‡†å¤‡ï¼ˆè®­ç»ƒé›†ï¼‰
-------------

*   åˆæˆæ•°æ®ï¼ˆBlender scenesï¼‰ç”¨äºå­¦ä¹ å°ºåº¦/å‡ ä½•å¤šæ ·æ€§ï¼›
*   çœŸå®æ•°æ®é›†ï¼šMegaDepth, ScanNet, Tanks&Temples, CO3D ç­‰ï¼›
*   å¯¹äºåŠ¨æ€ï¼ˆMonST3Rï¼‰éœ€è¦è§†é¢‘å¸§åºåˆ— + GT æˆ–åˆæˆè¿åŠ¨æ•°æ®ã€‚[arXiv+1](https://arxiv.org/abs/2312.14132?utm_source=chatgpt.com)

* * *

3.2 æ¨¡å‹æ­å»ºï¼ˆä»£ç çº§åˆ«å»ºè®®ï¼‰
----------------

1.  æ„å»º `Backbone`ï¼ˆViT / ResNetï¼‰è¿”å› `F`.
2.  å®ç° `CrossAttentionBlock`ï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼Œæ”¯æŒ maskï¼‰ã€‚
3.  æ„å»º `PointmapHead` ä¸ `PoseHead`ã€‚
4.  æ„å»º `LossModule`ï¼ˆpoint/depth/pose/align/confidenceï¼‰ã€‚
5.  ï¼ˆå¯é€‰ï¼‰å®ç° `GlobalAlign` æ¨¡å—ï¼Œåšæ‰¹æ¬¡å†…æˆ– epoch å†… alignmentã€‚

**ä¼ªç»“æ„**:

```
class Model(nn.Module):
    def __init__(...):
        self.backbone = ViT(...)
        self.attn_blocks = [CrossTransformer(...) for _ in range(L)]
        self.point_head = ConvHead(...)
        self.pose_head = MLPHead(...)
        self.conf_head = ConvHead(...)
    def forward(Ia, Ib):
        Fa = self.backbone(Ia)
        Fb = self.backbone(Ib)
        for block in self.attn_blocks:
            Fa = block(Fa, Fb)
            Fb = block(Fb, Fa)
        Pa = self.point_head(Fa)
        Pb = self.point_head(Fb)
        pose_ab = self.pose_head(global_pool(Fa, Fb))
        conf = self.conf_head(Fa)
        return Pa, Pb, pose_ab, conf
```

* * *

3.3 è®­ç»ƒå¾ªç¯ä¸ schedule
------------------

*   **Optimizer**: AdamW, lr = 1e-4 (backbone small lr 1e-5 if pretrained).
*   **Batch**: 8â€“32 pair images per GPU (å–å†³æ˜¾å­˜)ã€‚
*   **Scheduler**: cosine decay + warmup (warmup steps ~5k).
*   **Epochs**: 30â€“200ï¼Œè§†æ•°æ®è§„æ¨¡ã€‚
*   **Mixed precision**: ä½¿ç”¨ AMPï¼ˆfp16ï¼‰åŠ é€Ÿã€‚

* * *

3.4 æŸå¤±æƒé‡å»ºè®®ï¼ˆèµ·å§‹ï¼‰
--------------

*   Î»\_point = 1.0
*   Î»\_align = 1.0
*   Î»\_pose = 0.1
*   Î»\_smooth = 0.01
*   Î»\_conf\_reg = 0.1

è¿™äº›å€¼éœ€è¦åœ¨ç›®æ ‡æ•°æ®é›†ä¸Šåšå°èŒƒå›´æœç´¢ã€‚

* * *

3.5 æ¨ç†ä¸åå¤„ç†
----------

*   æ¨ç†æ—¶ç›´æ¥ output pointmaps + posesï¼›
*   è‹¥æœ‰å¤šå¼ å›¾ï¼šå…ˆåš pairwise æ¨ç†å¾—åˆ°å¤šç»„ pointmaps + pairwise pose â†’ ç”¨ GlobalAlign å°†æ‰€æœ‰ç‚¹ç»Ÿä¸€åˆ° common frameï¼ˆweighted ICP/Umeyamaï¼‰ï¼›DUSt3R çš„ global alignment å¯ä½œä¸ºå‚è€ƒå®ç°ã€‚[arXiv](https://arxiv.org/abs/2312.14132?utm_source=chatgpt.com)

* * *

4 â€” å„æ–¹æ³•çš„å…·ä½“å·®å¼‚åŒ–å®ç°ï¼ˆé‡è¦ï¼‰
===================

ä¸‹é¢æŠŠ DUSt3R / MonST3R / VGGT / MapAnything çš„ç‹¬åˆ°ç‚¹æ‹†å¼€æ¥ï¼Œå‘Šè¯‰ä½ **ä»£ç ä¸Šåº”è¯¥æ€ä¹ˆæ”¹**æ‰èƒ½å¾—åˆ°å¯¹åº”ç‰¹æ€§ã€‚

* * *

4.1 DUSt3Rï¼ˆæ ¸å¿ƒï¼špointmap è¡¨ç¤º + æ— å†…å‚/æ— å§¿æ€ä¹Ÿå¯ï¼‰
--------------------------------------

**æ ¸å¿ƒæ€æƒ³**ï¼šæŠŠ pairwise ä»»åŠ¡è½¬ä¸ºå¯¹æ¯åƒç´ å›å½’ç‚¹ï¼ˆpointmapï¼‰ï¼Œæ— éœ€å·²çŸ¥ç›¸æœºå†…å‚æˆ–ç²¾ç¡®ä½å§¿ï¼Œç½‘ç»œåœ¨é¢„æµ‹æ—¶å†…å«éšå¼å°ºåº¦ä¸æŠ•å½±é€»è¾‘ï¼Œä¹‹åå†ç”¨å¯¹é½æŠŠ pairwise ç»“æœåˆå¹¶ã€‚[arXiv](https://arxiv.org/abs/2312.14132?utm_source=chatgpt.com)

**å®ç°è¦ç‚¹**ï¼š

*   PointmapHead ç›´æ¥è¾“å‡º 3D XYZï¼ˆç›¸æœºå±€éƒ¨åæ ‡æˆ– canonical frameï¼‰è€Œéä»… depthã€‚
*   è®­ç»ƒæ—¶ä½¿ç”¨åŒå‘å¯¹é½æŸå¤±ï¼ˆAâ†’B, Bâ†’Aï¼‰ã€‚
*   æä¾› per-pixel confidence è¾“å‡ºå¹¶åœ¨ global align ä¸­ç”¨ä½œåŠ æƒã€‚
*   è‹¥å†…å‚æœªçŸ¥ï¼šä¸è¦åœ¨æŸå¤±é‡Œå¼ºåˆ¶ç”¨ Kï¼›è€Œæ˜¯è®©ç½‘ç»œå­¦ä¼šç›´æ¥å›å½’ç›¸å¯¹3Dï¼ˆpaper å·²å±•ç¤ºå¯è¡Œï¼‰ã€‚[arXiv](https://arxiv.org/abs/2312.14132?utm_source=chatgpt.com)

* * *

4.2 MonST3Rï¼ˆMotion-aware DUSt3Rï¼šåŠ¨æ€åœºæ™¯ï¼‰
-------------------------------------

**æ ¸å¿ƒæ€æƒ³**ï¼šåœ¨åŠ¨æ€åœºæ™¯ä¸­ï¼Œå•çº¯æŒ‰ç›¸æœºä½å§¿å¯¹é½ä¼šå¤±è´¥ï¼›MonST3R å¼•å…¥æ—¶åºå»ºæ¨¡ä¸ per-timestep pointmapsï¼Œä½¿æ¨¡å‹èƒ½å¤„ç†ç‰©ä½“è¿åŠ¨ä¸éåˆšä½“å˜å½¢ã€‚[arXiv](https://arxiv.org/html/2410.03825v2?utm_source=chatgpt.com)

**å®ç°è¦ç‚¹**ï¼š

*   è¾“å…¥ï¼šè¿ç»­è§†é¢‘å¸§åºåˆ—ï¼ˆI\_{tâˆ’k...t...t+k}ï¼‰ã€‚
*   å¢åŠ  temporal attention / temporal transformerï¼ˆæˆ–æ—¶åºå·ç§¯ï¼‰ï¼Œä½¿ token åœ¨ time è½´ä¸Šäº¤æµã€‚
*   è¾“å‡ºï¼šæ¯å¸§ pointmap ä¸ per-point velocity/flow æˆ– per-point time indexï¼ˆç”¨äºå»ºç«‹ 4D ç‚¹å›¾ï¼‰ã€‚
*   æŸå¤±ï¼šåŠ å…¥æ—¶é—´ä¸€è‡´æ€§æŸå¤±ï¼ˆflow supervision æˆ– photometric reprojection across timeï¼‰ï¼Œå¹¶å¯¹åŠ¨æ€ç‚¹ä½¿ç”¨ä¸“é—¨çš„ motion mask/flow headã€‚
*   å·¥ç¨‹ï¼šå®æ—¶æ¨¡å¼ä¸‹æä¾› fast feed-forward modeï¼ˆåºåˆ—æ»‘åŠ¨çª—å£ +åªè®¡ç®—Î”ï¼‰ã€‚[MonST3R Project+1](https://monst3r-project.github.io/?utm_source=chatgpt.com)

* * *

4.3 VGGTï¼ˆVisual Geometry Grounded Transformer â€” å¤§è§„æ¨¡ã€å¤šä»»åŠ¡ã€å¤šè§†ï¼‰
-----------------------------------------------------------

**æ ¸å¿ƒæ€æƒ³**ï¼šåšæˆä¸€ä¸ªâ€œå‡ ä½• foundation modelâ€â€”â€”ä¸€ä¸ª backbone èƒ½å¤Ÿä» 1â†’N å¼ å›¾ä¸­é¢„æµ‹ç›¸æœºå‚æ•°ã€depthã€point tracksã€3Dç‚¹è¿½è¸ªç­‰å¤šä»»åŠ¡è¾“å‡ºï¼›å¼ºè°ƒ**é¢„è®­ç»ƒ + å¤šä»»åŠ¡è”åˆå­¦ä¹ **ï¼Œå¹¶æ”¯æŒ hundreds of viewsã€‚[arXiv+1](https://arxiv.org/html/2503.11651v1?utm_source=chatgpt.com)

**å®ç°è¦ç‚¹**ï¼š

*   **Backbone**ï¼šå¤§è§„æ¨¡ ViTï¼ˆå¤§å‚æ•°ï¼‰ + strong pretrainingï¼ˆDINOv2 / MAEï¼‰ä»¥è·å¾—å‡ ä½•æ„ŸçŸ¥è¡¨å¾ã€‚[arXiv](https://arxiv.org/html/2503.11651v1?utm_source=chatgpt.com)
*   **Alternating attention**ï¼šframe-wise å’Œ global attention äº¤æ›¿ï¼Œç”¨äºæ‰©å±•åˆ°å¾ˆå¤šè§†å›¾ï¼ˆé™ä½ O(NÂ²) å¤æ‚åº¦ï¼‰ã€‚
*   **Multi-head outputs**ï¼šé™¤äº† pointmap å’Œ poseï¼ŒåŒæ—¶è¾“å‡º 3D point tracksï¼ˆè·¨ frames çš„ä¸€è‡´æ€§ idï¼‰ä¸ instance/semantic headsï¼ˆåšå¯ç¼–è¾‘åœºæ™¯ï¼‰ã€‚
*   **é¢„è®­ç»ƒ + finetune**ï¼šå…ˆç”¨å¤§è§„æ¨¡æ··åˆåˆæˆ/çœŸå®æ•°æ®åš self-supervised / supervised é¢„è®­ç»ƒï¼Œå†é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡ finetuneã€‚VGGT çš„å®ç°é‡å®ç”¨ï¼ˆ<1s é‡å»ºï¼‰å¹¶æœ‰ code releaseã€‚[GitHub+1](https://github.com/facebookresearch/vggt?utm_source=chatgpt.com)

* * *

4.4 MapAnythingï¼ˆFactored metric 3D reconstruction; æ”¯æŒå¤–éƒ¨å‡ ä½•è¾“å…¥ï¼‰
------------------------------------------------------------

**æ ¸å¿ƒæ€æƒ³**ï¼šæŠŠè¾“å‡º**å› å¼åˆ†è§£**ä¸ºï¼šlocal rays + depth maps + camera posesï¼Œèƒ½å¤Ÿç›´æ¥è¾“å‡º metric geometryï¼ˆå¸¦å°ºåº¦ï¼‰å¹¶æ”¯æŒä»»æ„è¾“å…¥ï¼ˆå›¾åƒã€éƒ¨åˆ†é‡å»ºã€ç›¸æœº infoï¼‰ã€‚é€‚åˆ mappingï¼ˆåŸå¸‚ asset geolocationï¼‰ã€‚[map-anything.github.io+1](https://map-anything.github.io/assets/MapAnything.pdf?utm_source=chatgpt.com)

**å®ç°è¦ç‚¹**ï¼š

*   **è¾“å…¥ token æ‰©å±•**ï¼šåœ¨ transformer ä¸­æŠŠ intrinsics / coarse pose / partial reconstructions ä¹Ÿä½œä¸º token æ³¨å…¥ï¼ˆä½¿æ¨¡å‹èƒ½åˆ©ç”¨å¤–éƒ¨ä¿¡æ¯ï¼‰ï¼›
*   **Metric supervision**ï¼šæ˜ç¡®åŠ å…¥åœ°é¢å®æµ‹å°ºåº¦ç›‘ç£ï¼ˆGPS / LiDARï¼‰æˆ–åˆæˆåœºæ™¯ metric supervisionï¼Œè®­ç»ƒç½‘ç»œè¾“å‡ºç»å¯¹å°ºåº¦é‡ã€‚
*   **Output factorization**ï¼šæŠŠè¾“å‡ºæ‹†ä¸º `(depth_map, local_ray_directions, camera_pose, scale)`ï¼Œè¿™æ ·å¯ä»¥ç”¨ç®€å•çš„å‡ ä½•å…¬å¼æŠŠ pixelâ†’geoï¼ˆlat/lonï¼‰è½¬æ¢ï¼ˆç”¨äºåŸå¸‚ mappingï¼‰ã€‚[map-anything.github.io](https://map-anything.github.io/assets/MapAnything.pdf?utm_source=chatgpt.com)

* * *

5 â€” æ•°å­¦å…³é”®ç‚¹ä¸æ¨å¯¼ï¼ˆç®€æ´ç‰ˆï¼‰
=================

ä¸‹é¢ç»™å‡ºå‡ ä¸ªä½ åœ¨å¤ç°ä¸­ä¼šé¢‘ç¹ç”¨åˆ°çš„æ•°å­¦å…¬å¼ï¼ˆéƒ½å·²åœ¨ä¸Šæ–‡æ¨¡å—ä¸­å¼•ç”¨ï¼Œè¿™é‡Œæ•´åˆä¸ºâ€œå·¥å…·ç®±â€ï¼‰ã€‚

*   **Cross-attention**:
    $$
    \mathrm{Attn}(Q,K,V)=\mathrm{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V
    $$
*   **Pointmap åæŠ•å½±ï¼ˆè‹¥ç”¨ depth è¡¨ç¤ºï¼‰**:
    $$
    X = D(u,v) K^{-1} [u, v, 1]^\top
    $$
*   **Pairwise alignment loss**ï¼ˆåˆšä½“ï¼‰:
    $$
    L_{align} = \sum_{i} w_i \| P^B_i - (R P^A_i + t) \|_1
    $$
*   **Rotation distanceï¼ˆgeodesicï¼‰**:
    $$
    d_{rot}(R_1,R_2) = \frac{1}{\sqrt 2} \| \log(R_1 R_2^\top) \|_F
    $$
*   **Confidence-weighted loss**:
    $$
    L = \sum_i c_i \| \hat p_i - p_i^{gt} \|_1 + \lambda \,\mathrm{Reg}(c)
    $$

* * *

6 â€” è®­ç»ƒæ•°æ®é›†ã€è¯„æµ‹ä¸æŒ‡æ ‡
===============

**æ•°æ®é›†**ï¼š

*   åˆæˆï¼šBlenderã€DTU style synthetic scenesï¼ˆç”¨äºå°ºåº¦/shape diversityï¼‰
*   çœŸå®ï¼šMegaDepth, ScanNet, Tanks & Temples, CO3Dï¼ˆå®ä¾‹ã€éåˆšä½“ï¼‰, Waymo/Waymo Openï¼ˆè¡—æ™¯ & åŸå¸‚ mappingï¼‰
*   åŠ¨æ€ï¼šDAVIS / custom synthetic dynamic datasetsï¼ˆMonST3Rï¼‰[arXiv+1](https://arxiv.org/abs/2312.14132?utm_source=chatgpt.com)

**æŒ‡æ ‡**ï¼š

*   Depth: AbsRel, RMSE, Î´<1.25, scale-invariant metrics
*   Pose: rotation error (deg), translation error (m)
*   3D reconstruction: Chamfer distance, F-scoreï¼ˆpointcloudï¼‰
*   Tracking: MOTA / ID switchï¼ˆè‹¥æœ‰ point track è¾“å‡ºï¼‰
*   Geolocalization: geodesic distance / meters accuracyï¼ˆMapAnything ç‰¹åˆ«å…³æ³¨ï¼‰[map-anything.github.io](https://map-anything.github.io/assets/MapAnything.pdf?utm_source=chatgpt.com)

* * *

7 â€” å®ç°ä¸è°ƒå‚å®æˆ˜æŠ€å·§ï¼ˆå·¥ç¨‹è´´å£«ï¼‰
===================

1.  **å…ˆå¤ç°å°æ¨¡å‹**ï¼šç”¨å° ViT/ResNetã€è¾ƒå°‘ attention å±‚ã€ä½åˆ†è¾¨ç‡å›¾åƒæŠŠ pipeline èµ°é€šã€‚
2.  **é¢„è®­ç»ƒ backbone**ï¼šå‡ ä½•ä»»åŠ¡å¯¹ backbone å‡ ä½•æ•æ„Ÿï¼Œä½¿ç”¨ DINOv2 æˆ– MAE å¯¹æ€§èƒ½å¸®åŠ©å¾ˆå¤§ï¼ˆVGGT å¼ºè°ƒé¢„è®­ç»ƒçš„é‡è¦æ€§ï¼‰ã€‚[arXiv](https://arxiv.org/html/2503.11651v1?utm_source=chatgpt.com)
3.  **æ³¨æ„é®æŒ¡/ç½®ä¿¡è®­ç»ƒ**ï¼šåœ¨attentionä¸Šåšé®æŒ¡maskæˆ–ç”¨ç½®ä¿¡åº¦ head å‡è½»é”™è¯¯å¯¹åº”çš„è´Ÿé¢å½±å“ã€‚
4.  **æ··åˆç›‘ç£**ï¼šåˆæˆ + çœŸå®ï¼ˆæœ‰çœŸå® depth/pose çš„å­é›†ï¼‰æ··åˆè®­ç»ƒå¯æ˜¾è‘—æå‡æ³›åŒ–ã€‚
5.  **numerical stability**ï¼šå›å½’ depth/Z æ—¶ç”¨ `exp` æˆ– `softplus` ä¿è¯æ­£å€¼ï¼›rotation ç”¨ 6D æˆ– quaternion å‚æ•°åŒ–å¹¶æ­£åˆ™åŒ–ã€‚
6.  **multi-GPU & batch composition**ï¼šåŒä¸€ batch å†…åŒ…å«ç›¸ä¼¼è§†è§’å¯¹æ›´åˆ©äºå­¦ä¹ ï¼ˆhard negative sampling ä¹Ÿé‡è¦ï¼‰ã€‚
7.  **debug**ï¼šå¯è§†åŒ– attention çƒ­å›¾ã€pointmap æŠ•å½±åˆ° RGB å›¾åƒä¸Šï¼Œæ£€æŸ¥æ˜¯å¦åœ¨ä½çº¹ç†åŒºåŸŸå¤±é…ã€‚

* * *

8 â€” å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ³•ï¼ˆFAQï¼‰
==================

*   **Qï¼šæ¨¡å‹å¦‚ä½•å¤„ç†æœªçŸ¥å†…å‚ï¼Ÿ**  
    Aï¼šDUSt3R å±•ç¤ºäº†ä¸ä¾èµ–å†…å‚çš„ pointmap å›å½’ç­–ç•¥ï¼›å·¥ç¨‹ä¸Šå¯é€‰ä¸¤æ¡è·¯â€”â€”è®­ç»ƒæ—¶éšæœºåŒ–å†…å‚ä½¿æ¨¡å‹å­¦ä¼šä¸ä¾èµ–ï¼Œæˆ–åŒæ—¶å›å½’å†…å‚ headï¼ˆå¹¶ç”¨å°‘é‡æ•°æ®ç›‘ç£ï¼‰ã€‚[arXiv](https://arxiv.org/abs/2312.14132?utm_source=chatgpt.com)
*   **Qï¼šåœ¨æç¨€è§†ï¼ˆvery few viewsï¼‰æƒ…å†µä¸‹èƒ½å¦ç¨³å®šï¼Ÿ**  
    Aï¼šéœ€è¦å¼ºå…ˆéªŒï¼ˆpretraining + confidenceï¼‰ï¼›å¯ä»¥ç»“åˆ MapAnything çš„å› å¼åŒ–è¾“å…¥ï¼ˆä¼ å…¥ partial recon / pose hintï¼‰ä»¥æå‡ç¨³å®šæ€§ã€‚[arXiv](https://arxiv.org/abs/2509.13414?utm_source=chatgpt.com)
*   **Qï¼šå¦‚ä½•æŠŠè¿™äº›è¾“å‡ºç”¨äº NeRF / 3DGS åˆå§‹åŒ–ï¼Ÿ**  
    Aï¼šæŠŠ pointmap æŠ•å½±å‡ºçš„ç‚¹äº‘ä½œä¸º 3DGS çš„ Gaussian åˆå§‹åŒ–ï¼Œæˆ–æŠŠ depth + pose ä¼ å…¥ NeRF åšåˆå§‹åŒ–å¹¶ä½œä¸ºå…ˆéªŒä»è€ŒåŠ é€Ÿæ”¶æ•›ï¼ˆæ··åˆæ–¹æ³•å·²è¢«å°è¯•å¹¶è¯æ˜æœ‰æ•ˆï¼‰ã€‚[arXiv+1](https://arxiv.org/abs/2312.14132?utm_source=chatgpt.com)

* * *

9 â€” ç«¯åˆ°ç«¯å¤ç°æ¸…å•ï¼ˆä½ æ‹¿åˆ°ä»£ç è¦åšçš„äº‹ï¼‰
======================

1.  å‡†å¤‡æ•°æ®é›†ï¼ˆä¸‹è½½ MegaDepth/ScanNet ç­‰ï¼‰ï¼Œå†™ dataloader è¾“å‡º (I\_a, I\_b, K\_a?, K\_b?, gt\_depth?, gt\_pose?)ã€‚
2.  å®ç° backboneã€cross-attention blockã€pointmap/pose/conf headsï¼ˆå¦‚ä¸Šä¼ªç ï¼‰ã€‚
3.  å®ç°æŸå¤±å‡½æ•°ï¼ˆpoint/depth/align/pose/confidenceï¼‰ã€‚
4.  è®­ç»ƒï¼šå…ˆç”¨å°æ•°æ®é›†è¿‡æ‹Ÿåˆä¸€ä¸ªåœºæ™¯ï¼ˆéªŒè¯æ¨¡å‹èƒ½å­¦åˆ°ä¸œè¥¿ï¼‰ï¼Œå†æ‰©å±•åˆ°å¤§é›†è®­ã€‚
5.  å¯è§†åŒ–ï¼šattention mapsã€pointmap æŠ•å½±ã€ç½®ä¿¡åº¦çƒ­å›¾ã€‚
6.  æ‰©å±•ï¼šmulti-view global alignment / temporal window for MonST3R / factorized input for MapAnythingã€‚

* * *

10 â€” å‚è€ƒï¼ˆå¿«é€Ÿæ ¸éªŒä¸è¿›ä¸€æ­¥é˜…è¯»ï¼‰
===================

*   DUSt3R â€” Wang et al., _DUSt3R: Geometric 3D Vision Made Easy_, CVPR 2024 (paper + code). [CVFå¼€æ”¾è·å–+1](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_DUSt3R_Geometric_3D_Vision_Made_Easy_CVPR_2024_paper.pdf?utm_source=chatgpt.com)
*   MonST3R â€” _MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion_, ICLR 2025 (project page & paper). [arXiv+1](https://arxiv.org/html/2410.03825v2?utm_source=chatgpt.com)
*   VGGT â€” _VGGT: Visual Geometry Grounded Transformer_, CVPR 2025 (paper + code). [arXiv+1](https://arxiv.org/html/2503.11651v1?utm_source=chatgpt.com)
*   MapAnything â€” _MapAnything: Universal Feed-Forward Metric 3D Reconstruction_ (arXiv / project page). [map-anything.github.io+1](https://map-anything.github.io/assets/MapAnything.pdf?utm_source=chatgpt.com)

* * *

11 â€” æˆ‘å¯ä»¥é©¬ä¸Šä¸ºä½ åšçš„ä¸‰ä»¶äº‹ï¼ˆé€‰ä¸€ï¼‰
=====================

å¦‚æœä½ æƒ³è¦ï¼Œæˆ‘å¯ä»¥ç«‹åˆ»åšä¸‹é¢ä»»ä¸€ä»¶äº‹ï¼ˆç°åœ¨å°±ç”Ÿæˆï¼‰ï¼š  
A. æŠŠä¸Šé¢â€œå®Œæ•´å®ç°ä¼ªç â€æ‰©æˆå¯è¿è¡Œçš„ PyTorch skeletonï¼ˆä¸€ä¸ª repo çš„ `model.py + train.py + loss.py` çº§åˆ«ï¼Œå¯ç›´æ¥å¾®è°ƒï¼‰ã€‚  
B. ä»¥ DUSt3R ä¸ºæ¨¡æ¿ç”Ÿæˆå®Œæ•´å¤ç°è¯´æ˜ï¼ˆstep-by-stepï¼‰ï¼ŒåŒ…å« exact è¶…å‚ã€è®­ç»ƒ scheduleã€shell è„šæœ¬ä¸æ•°æ®å‡†å¤‡è„šæœ¬ã€‚  
C. æŠŠ DUSt3R â†’ MonST3R â†’ VGGT â†’ MapAnything çš„æ¶æ„å›¾ï¼ˆæ¨¡å—åŒ–æµç¨‹å›¾ï¼‰ç”»å‡ºæ¥å¹¶å¯¼å‡ºä¸º PNGï¼ˆå«å…³é”® tensor shapesï¼‰ã€‚

å›å¤ Aã€B æˆ– Cï¼ˆæˆ–åŒæ—¶å¤šä¸ªï¼‰ï¼Œæˆ‘**é©¬ä¸Š**æŠŠå¯¹åº”äº§å‡ºåšå‡ºæ¥ã€‚

## Prompt:
SIU3Ræ–¹æ³•å’Œä¸Šè¿°æ–¹æ³•æœ‰ä»€ä¹ˆåŒºåˆ«

5/5

## Response:
ä¸‹é¢æˆ‘æ¥å›ç­”ä½ å…³äº **SIU3R æ–¹æ³•** ä¸ **DUSt3R / MonST3R / VGGT / MapAnything** è¿™ä¸€ç±» **å‰é¦ˆå¼å‡ ä½•é‡å»ºæ¨¡å‹** çš„åŒºåˆ«ã€‚

ç”±äºä½ æ²¡æœ‰å…·ä½“æŒ‡æ˜ â€œSIU3Râ€ çš„å‡ºå¤„ï¼ˆæ¯”å¦‚è®ºæ–‡åæˆ–é¡¹ç›®åï¼‰ï¼Œæˆ‘æ¨æµ‹ SIU3R **å¯èƒ½æŒ‡çš„æ˜¯**ï¼š

> **S**ingle-**I**mage **U**nilateral **3R**econstruction / **S**elf-supervised **I**terative **U**nified **3R**econstruction  
> æˆ–è€…æ˜¯ä¸€ç§æœ€è¿‘æå‡ºçš„ç±»ä¼¼ â€œ3R ç³»åˆ—â€ å‘½åï¼ˆReconstruct / Register / Refineï¼‰

å¦‚æœä½ èƒ½æä¾› SIU3R çš„è®ºæ–‡é“¾æ¥æˆ‘å¯ä»¥æ›´ç²¾ç¡®å®šä¹‰ã€‚ä½†ç›®å‰ä»å‘½åè§„å¾‹åˆ¤æ–­ï¼ŒSIU3R åº”è¯¥ä¸ **å•ç›® æˆ– æ— é…å‡†è¾“å…¥ çš„ 3D é‡å»ºæ–¹æ³•** ç›¸å…³ï¼Œç±»ä¼¼äºï¼š

| æ–¹æ³• | è¾“å…¥ | è¾“å‡º | æ˜¯å¦éœ€è¦é…å¯¹å›¾åƒ | ååŒæœºåˆ¶ |
| --- | --- | --- | --- | --- |
| **DUSt3R** | åŒå›¾æˆ–å¤šå›¾ | PointMap + Pose | âœ… éœ€è¦ä¸¤å¼ è§†å›¾ | Cross-Attention |
| **MonST3R** | å¤šå¸§è§†é¢‘ï¼ˆåŠ¨æ€ï¼‰ | 4D æ—¶åº PointMap | âœ… éœ€è¦å¤šè§†å›¾ | Temporal Attention |
| **VGGT** | å¤šå›¾ï¼ˆä»»æ„ï¼‰ | Depth + Pose + Track + 3D | âœ… å¤šè§† | Multi-task Transformer |
| **MapAnything** | å¤šæ¨¡æ€(å›¾åƒ+pose+hint) | Metric Depth + GeoPose | âœ… å¯è¾…åŠ© | Factored Prediction |
| **SIU3Rï¼ˆæ¨æµ‹ï¼‰** | å•å›¾ / æœªé…å‡†åºåˆ— | Depth æˆ– Canonical 3D | âŒ å¯ä¸éœ€è¦ç¬¬äºŒå¼ å›¾ | å¯èƒ½æ˜¯ Prior-based |

* * *

âœ… æ‰€ä»¥ï¼Œ**SIU3R ä¸ä¸Šè¿°æ–¹æ³•çš„æœ€å¤§åŒºåˆ«**å¯èƒ½ä½“ç°åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š
=====================================

| æ¯”è¾ƒç»´åº¦ | SIU3Rï¼ˆæ¨æµ‹ï¼‰ | DUSt3R / MonST3R / VGGT / MapAnything |
| --- | --- | --- |
| **æ˜¯å¦ä¾èµ–å¤šè§†å›¾ï¼Ÿ** | âŒ _å¯èƒ½æ”¯æŒå•å›¾è¾“å…¥_ | âœ… å¤§å¤šæ•°ä¾èµ–ä¸¤å›¾æˆ–å¤šå›¾äº’ç›¸å¯¹é½ |
| **æ˜¯å¦æ˜¾å¼åŒ¹é…è·¨è§†è§’ï¼Ÿ** | âŒ æ— åŒ¹é…ï¼ˆå¯èƒ½åŸºäºshape prior / diffusionï¼‰ | âœ… ä¾èµ– Cross-Attention å»ºç«‹ correspondence |
| **å‡ ä½•æ¥æº** | âœ… _å…ˆéªŒé©±åŠ¨_ï¼ˆlearned shape / diffusion / implicit biasï¼‰ | âœ… _è§‚æµ‹é©±åŠ¨_ï¼ˆtriangulation via epipolar cuesï¼‰ |
| **è¾“å‡ºåæ ‡ç³»** | â“ _Canonical / normalized space_ | âœ… _ç›¸æœºåæ ‡ / metric ç©ºé—´_ |
| **è§†å·® / æ·±åº¦æ¥æº** | âŒ æ— çœŸå®è§†å·®ï¼ˆåªèƒ½çŒœæ·±åº¦ï¼‰ | âœ… åŸºäºè§†å·®ä¿¡æ¯çœŸå®ä¼°è®¡ |
| **é²æ£’æ€§ & ç²¾åº¦** | â­ å¼ºå…ˆéªŒä½†å¯èƒ½å¤±çœŸ | â­ ç²¾ç»†å‡ ä½•ä½†ä¾èµ–åŒ¹é…è´¨é‡ |

* * *

ğŸ§  æ ¸å¿ƒæœ¬è´¨åŒºåˆ«ä¸€å¥è¯æ€»ç»“ï¼š
===============

> **SIU3R æ›´åƒæ˜¯â€œå•ç›®å…ˆéªŒé©±åŠ¨å‹é‡å»ºâ€**ï¼Œé çš„æ˜¯**ç½‘ç»œå­¦åˆ°çš„é€šç”¨ 3D ç»“æ„ç»éªŒ**ï¼›
> 
> è€Œ **DUSt3R / MonST3R / VGGT / MapAnything æ˜¯â€œå‡ ä½•çº¦æŸé©±åŠ¨å‹é‡å»ºâ€**ï¼Œé çš„æ˜¯**çœŸå®è§†è§’å·®ã€åƒç´ åŒ¹é…å’Œ rigid alignment**ã€‚



## Prompt:
1.2 æ¨¡å‹æ„æˆï¼æ¨¡å—
ç‰¹å¾å¯¹é½
Unified queries
pixel-aligned representation
Râ†’U, Uâ†’R
è¯¦è§£ä¸Šè¿°æ¦‚å¿µ

## Response:
å¥½â€”â€”ä¸‹é¢æŠŠä½ åˆ—å‡ºçš„å‡ ä¸ªæ ¸å¿ƒæ¦‚å¿µ **ï¼ˆâ€œæ¨¡å‹æ„æˆ/æ¨¡å—ã€ç‰¹å¾å¯¹é½ã€Unified queriesã€pixel-aligned representationã€Râ†’Uã€Uâ†’Râ€ï¼‰** ä¸€ä¸€æ‹†æ¸…æ¥šã€ä¸¥è°¨å®šä¹‰ã€ç»™å‡ºæ•°å­¦/ä¼ªç å®ç°ã€å¹¶è¯´æ˜å·¥ç¨‹å®ç°è¦ç‚¹ä¸å¸¸è§å˜ä½“ã€‚å†…å®¹æ—¢è®²ç›´è§‰ä¹Ÿç»™å…·ä½“å®ç°ç»†èŠ‚ï¼ˆtensor shapesã€attention å…¬å¼ã€loss ç”¨æ³•ï¼‰ï¼Œæ–¹ä¾¿ä½ æŠŠå®ƒä»¬æ•´åˆè¿› DUSt3R / VGGT / MapAnything è¿™ç±»ç³»ç»Ÿé‡Œå¤ç°æˆ–æ”¹è¿›ã€‚

æˆ‘å…ˆç»™å‡ºä¸€ä¸ªçŸ­è¦ç‚¹æ€»ç»“ï¼Œç„¶åå±•å¼€é€æ¡è¯¦è§£ï¼ˆæ¯ä¸€èŠ‚éƒ½ä¼šåŒ…å«ï¼šç›´è§‰ + æ•°å­¦è¡¨ç¤º + ä¼ªç /å®ç°æç¤º + è°ƒå‚/å·¥ç¨‹å»ºè®®ï¼‰ã€‚

* * *

ä¸€è¨€ä»¥è”½ä¹‹ï¼ˆå…ˆçœ‹ç»“è®ºï¼‰
-----------

*   **æ¨¡å‹æ„æˆ/æ¨¡å—**ï¼šé€šå¸¸ç”± _Image Encoder (per-view pixel tokens)_ â†’ _Cross-view Fusion (attention / correlation)_ â†’ _Unified Query Bank (U)_ â†’ _Pixel-aligned decoders (pointmap / depth / confidence heads)_ â†’ _Global alignment / pose estimation_ ç»„æˆã€‚
*   **ç‰¹å¾å¯¹é½**ï¼šæŠŠä¸åŒè§†å›¾çš„ feature æ”¾åˆ°ä¸€ä¸ªå…±åŒå‚è€ƒï¼ˆç©ºé—´æˆ– feature ç©ºé—´ï¼‰ä»¥ä¾¿æ¯”è¾ƒ/èåˆï¼›æ—¢æœ‰ _soft attention å¯¹é½_ï¼Œä¹Ÿæœ‰ _å‡ ä½•æ˜¾å¼æŠ•å½±å¯¹é½_ï¼ˆç”¨å†…å‚/poseï¼‰ã€‚
*   **Unified queries (U)**ï¼šä¸€ç»„ learnable / data-derived query tokensï¼Œä»£è¡¨åœºæ™¯ä¸­â€œè¦ä¼°è®¡â€çš„ 3D åŸå­ï¼ˆç‚¹ / ray / patch / objectï¼‰ï¼›U ä¸æ¯ä¸ª view çš„åƒç´  tokensï¼ˆRï¼‰é€šè¿‡ä¸¤ç§æ–¹å‘çš„ attention äº¤äº’å®ç°ä¿¡æ¯äº¤æ¢ã€‚
*   **Pixel-aligned representation**ï¼šä¿ç•™åƒç´ ç©ºé—´ä½ç½®ï¼ˆ2D â†’ per-pixel featureï¼‰å¹¶ç¡®ä¿è¿™äº› feature èƒ½ç²¾ç¡®åæŠ•å½±åˆ° 3Dï¼ˆç”¨äºç‚¹å›¾/æ·±åº¦/éšå¼å‡½æ•°ï¼‰ï¼›å¸¸åšæ³•æ˜¯æŠŠåƒç´ åæ ‡/ç›¸æœºå‚æ•°ä¸€å¹¶ç¼–ç åˆ° featureã€‚
*   **Râ†’U / Uâ†’R**ï¼šä¸¤ç§ attention æ¨¡å¼â€”â€” _Râ†’U_ï¼ˆç”¨ View Tokens R åš keys/valuesï¼Œç”¨ U åš queriesï¼ŒæŠŠ view ä¿¡æ¯â€œå†™å…¥â€ unified queriesï¼‰å’Œ _Uâ†’R_ï¼ˆç”¨ U åš keys/valuesï¼Œç”¨ R åš queriesï¼ŒæŠŠå…¨å±€ä¿¡æ¯â€œå¹¿æ’­/å›å†™â€åˆ°åƒç´ çº§è¡¨ç¤ºï¼‰ã€‚  
    è¿™äº›æ˜¯ç°ä»£å¤šè§†å‡ ä½•-transformer çš„æ ¸å¿ƒäº¤äº’èŒƒå¼ã€‚

* * *

æ¥ä¸‹æ¥é€é¡¹è¯¦è§£ã€‚

* * *

1 æ¨¡å‹æ„æˆ / æ¨¡å—ï¼ˆå®Œæ•´åˆ†å±‚ä¸æ•°æ®æµï¼‰
=====================

### 1.1 æ¨¡å‹é«˜å±‚æµæ°´ï¼ˆæ¦‚è§ˆï¼‰

```
Input: N RGB images {I_n}, optional intrinsics {K_n}, optional coarse poses
â†“
(1) Image Encoder (per image) -> pixel tokens R_n âˆˆ â„^{H'Ã—W'Ã—C}
â†“
(2) Positional / Camera Encoding -> attach UV coords / ray dirs / K to tokens
â†“
(3) Cross-view Fusion blocks (alternating attention)ï¼š
      - R â†” R cross-attentionï¼ˆpairwise matchingï¼‰æˆ– R â†” U ï¼ˆè§ä¸‹ï¼‰
â†“
(4) Unified Query Bank (U) (learnable or derived) -> U âˆˆ â„^{MÃ—D}
â†“
(5) Iterative Râ†’U and Uâ†’R blocks (å¤šå±‚)ï¼ˆæŠŠè§†å›¾ä¿¡æ¯èšåˆåˆ° Uï¼Œå†æŠŠ U çš„å…¨å±€åœºæ™¯çŸ¥è¯†å†™å› Rï¼‰
â†“
(6) Decoder Heads:
      - Pointmap head: per-pixel â†’ 3D (X,Y,Z)
      - Depth head: per-pixel â†’ depth
      - Pose head: global pooled -> R|t
      - Confidence / Occlusion head
â†“
(7) (Optional) Global alignment / fusion across many pairs -> produce unified point cloud
```

### 1.2 å„æ¨¡å—ç»†èŠ‚ï¼ˆå®ç°/shapeï¼‰

*   **Encoder**ï¼šViT/ResNet â†’ æ¯å¼ å›¾å˜æˆ `R_n` of shape `[H' * W', C]`ï¼ˆtoken listï¼‰æˆ– `[C, H', W']`ã€‚
*   **Positional / Camera Encoding**ï¼šä¸ºæ¯ token concatenation æˆ– add ç›¸æœºå†…å‚ç¼–ç ã€åƒç´  UVã€ray directionã€‚
*   **Unified queries U**ï¼š`U âˆˆ â„^{MÃ—D}`ï¼ˆM å¯ä¸ºç½‘æ ¼ç‚¹æ•°ã€learnable slotsã€æˆ–æŒ‰ç¨€ç–å…³é”®ç‚¹æ•°è®¾å®šï¼‰ã€‚
*   **Attention block**ï¼šæ ‡å‡† multi-head attentionï¼ˆMHAï¼‰å®ç° Râ†’U / Uâ†’Rï¼ˆè¯¦è§ä¸‹ï¼‰ã€‚

* * *

2 ç‰¹å¾å¯¹é½ï¼ˆFeature Alignmentï¼‰
=========================

### 2.1 ç›´è§‰

ç‰¹å¾å¯¹é½è§£å†³çš„é—®é¢˜æ˜¯ï¼š**ä¸åŒè§†å›¾çš„ç‰¹å¾åœ¨ç©ºé—´ä¸è¯­ä¹‰ä¸Šå¹¶ä¸å¤©ç„¶å¯æ¯”**ã€‚å¯¹é½çš„ç›®çš„å°±æ˜¯æŠŠå®ƒä»¬æ”¾åˆ°åŒä¸€ä¸ªå‚è€ƒç³»æˆ–å»ºç«‹å¯æ¯”è¾ƒçš„å¯¹åº”å…³ç³»ï¼Œä½¿å¾—è·¨è§†å›¾èåˆæœ‰æ„ä¹‰ï¼ˆä¾‹å¦‚ä¸‰è§’åŒ–ã€è§†å·®ä¼°è®¡ã€ç‚¹ä¸€è‡´æ€§ï¼‰ã€‚

### 2.2 ä¸¤ç±»å¸¸è§å¯¹é½æ–¹å¼

1.  **æ˜¾å¼å‡ ä½•å¯¹é½ï¼ˆprojective / epipolarï¼‰**
    *   åˆ©ç”¨ç›¸æœºå†…å‚  $K$  ä¸ç›¸å¯¹ä½å§¿  $T$  æŠŠä¸€ä¸ªè§†å›¾çš„åƒç´ æŠ•å½±åˆ°å¦ä¸€ä¸ªè§†å›¾çš„åƒç´ å¹³é¢ï¼ˆæˆ– epipolar lineï¼‰ï¼Œåœ¨æŠ•å½±ä½ç½®é‡‡æ ·å…¶ feature â†’ æ„æˆå¯¹é½ã€‚
    *   ä¼˜ç‚¹ï¼šå‡ ä½•æ˜ç¡®ã€å¯¹é®æŒ¡å¯åˆ¤å®šï¼›ç¼ºç‚¹ï¼šéœ€è¦ pose / intrinsicsï¼Œæˆ–ä¼°è®¡å®ƒä»¬ã€‚
    *   æ•°å­¦ï¼šç»™åƒç´   $p_A$ ï¼ˆåœ¨ A å›¾åƒï¼‰ï¼Œå¦‚æœæ·±åº¦ä¸º zï¼Œåˆ™å…¶ 3D ç‚¹  $X = z K_A^{-1} [u,v,1]$ ï¼Œåœ¨ B ä¸Šçš„æŠ•å½±  $p_B = K_B [R|t] X$ ã€‚
2.  **éšå¼/soft å¯¹é½ï¼ˆattention/correlationï¼‰**
    *   ç”¨ç›¸ä¼¼åº¦çŸ©é˜µæˆ– cross-attention å­¦ä¹ ä¸€ä¸ªâ€œsoftâ€åŒ¹é…æƒé‡  $W_{ij}$  è¡¨å¾ token iï¼ˆview Aï¼‰ä¸ token jï¼ˆview Bï¼‰çš„å…³ç³»ã€‚
    *   å…¬å¼ï¼ˆç›¸ä¼¼åº¦ï¼‰ï¼š  $S_{ij} = \exp( \langle f_i, f_j \rangle / \tau )$ ã€‚
    *   è¿™å°±æ˜¯ DUSt3R/VGGT ä¸­ç”¨ cross-attention å»ºç«‹çš„å¯¹é½ï¼šç½‘ç»œè‡ªå·±å­¦ä¹  "å“ªä¸ªåƒç´ åŒ¹é…å“ªä¸ªåƒç´ "ã€‚

### 2.3 ä½•æ—¶é€‰å“ªç§ï¼Ÿ

*   æœ‰å¯é ç›¸æœº/pose â†’ ä¼˜å…ˆæ˜¾å¼å‡ ä½•å¯¹é½ï¼ˆæ•ˆç‡ã€å¯è§£é‡Šï¼‰ã€‚
*   æ²¡æœ‰ç›¸æœº/pose æˆ–å¸Œæœ›ç«¯åˆ°ç«¯å­¦ä¹  â†’ ç”¨ attention-based soft å¯¹é½ï¼ˆé²æ£’åˆ°é®æŒ¡/åå°„ï¼Œä½†æ›´ä¾èµ–æ•°æ®ä¸æ­£åˆ™åŒ–ï¼‰ã€‚

### 2.4 å®ç°æŠ€å·§ï¼ˆå·¥ç¨‹ï¼‰

*   æ˜¾å¼å¯¹é½ä¸­åš **bilinear sampling**ï¼ˆgrid\_sampleï¼‰ä»¥åœ¨ feature map ä¸Šå–å€¼ï¼›å¯¹ epipolar line ä¸Šçš„é‡‡æ ·å¯ç”¨å¤šç‚¹é‡‡æ ·å¹¶æ±‡èšã€‚
*   éšå¼å¯¹é½ä¸­é™åˆ¶ attention èŒƒå›´ï¼ˆepipolar-aware attention / windowed attentionï¼‰å¯åŠ é€Ÿä¸ç¨³å®šã€‚
*   å¯¹é½åæœ€å¥½é¢„æµ‹ **confidence**ï¼ˆæˆ– maskï¼‰æ¥æŠ‘åˆ¶é”™è¯¯çš„å¯¹åº”ã€‚

* * *

3 Pixel-aligned representationï¼ˆåƒç´ å¯¹é½è¡¨ç¤ºï¼‰
======================================

### 3.1 å®šä¹‰ä¸ç›´è§‰

**Pixel-aligned representation** æŒ‡çš„æ˜¯æ¯ä¸ªåƒç´ ï¼ˆæˆ–åƒç´ çº§ tokenï¼‰ä¿æŒå…¶ç©ºé—´ä½ç½®ç´¢å¼•ï¼Œå¹¶æºå¸¦èƒ½ç›´æ¥ç”¨äº 3D åæŠ•å½±çš„ featureï¼ˆä¾‹å¦‚æŠŠåƒç´ åæ ‡ `u,v`ã€ray directionã€å±€éƒ¨æ·±åº¦ cue ç¼–ç è¿› featureï¼‰ã€‚è¿™ç§è¡¨ç¤ºæ–¹ä¾¿æŠŠ 2D ä¿¡æ¯ç›´æ¥æ˜ å°„å› 3Dï¼Œå¸¸ç”¨äºå•è§†åˆ° 3Dï¼ˆPIFuã€pixel-aligned implicit fieldsï¼‰ä¸ DUSt3R çš„ pointmap è¾“å‡ºã€‚

å…³é”®ç‰¹æ€§ï¼š

*   ä¿ç•™ç©ºé—´å¸ƒå±€ï¼ˆä¸æ˜¯å…¨å±€ pooled descriptorï¼‰ã€‚
*   å«æœ‰**ä½ç½®ç¼–ç **ï¼ˆåƒç´ åæ ‡ã€ç›¸æœº intrinsicsã€ray directionsï¼‰ã€‚
*   å¯ä»¥ç›´æ¥é€šè¿‡åæŠ•å½±å…¬å¼æŠŠ feature ä¸æ·±åº¦/pointmapç»“åˆï¼Œå¾—åˆ° 3Dã€‚

### 3.2 å…¸å‹ç¼–ç æ–¹å¼

å¯¹ token `f(u,v)`ï¼š

*   `f'(u,v) = concat( f(u,v), PE(u,v), PE(ray_direction(u,v)), camera_params )`
*   `PE` å¯ä»¥æ˜¯æ­£å¼¦ä½ç½®ç¼–ç æˆ–å¯å­¦ä¹ çš„åæ ‡ MLPï¼ˆcoord-MLPï¼‰ã€‚

### 3.3 ä¸ºä»€ä¹ˆé‡è¦

*   å½“ decoder éœ€è¦åœ¨**åƒç´ çº§**å›å½’ 3D ç‚¹ï¼ˆpointmapï¼‰æ—¶ï¼Œpixel-aligned feature èƒ½ä¿ç•™è¶³å¤Ÿçš„å±€éƒ¨å‡ ä½•ä¿¡æ¯ï¼Œé¿å…å…¨å±€ latent çš„æ¨¡ç³ŠåŒ–ã€‚
*   åœ¨å¤šè§†èåˆæ—¶ï¼Œpixel features å¯ç›´æ¥å‚ä¸å¯¹é½é‡‡æ ·æˆ– attentionï¼ˆæ›´è‡ªç„¶åœ°å…³è”åˆ°å…·ä½“åƒç´ ï¼‰ã€‚

### 3.4 å®è·µè¦ç‚¹

*   åœ¨ encoder è¾“å‡ºçš„æ¯ä¸ª token ä¸Šé™„å¸¦ `uv_norm âˆˆ [-1,1]^2` ä¸ `ray_dir`ï¼ˆè®¡ç®—æ–¹æ³•ï¼š`K^{-1}[u,v,1]` normalizeï¼‰ã€‚
*   ä½¿ç”¨ FPN/å¤šå°ºåº¦ fusionï¼šç»†èŠ‚ small scale + è¯­ä¹‰ large scale åŒæ—¶ä¿ç•™ã€‚
*   è‹¥éœ€è¦æŠ—å°ºåº¦ï¼šæŠŠç›¸å¯¹å°ºåº¦å½’ä¸€åŒ–ä¸ºè§†å›¾å†…å±€éƒ¨å°ºåº¦æˆ–ç”¨ `log(depth)` å‚æ•°åŒ–ã€‚

* * *

4 Unified Queriesï¼ˆUï¼‰ï¼šæ¦‚å¿µã€è§’è‰²ä¸å®ç°
=============================

### 4.1 ä»€ä¹ˆæ˜¯ Unified Queriesï¼ˆç›´è§‰ï¼‰

Unified queriesï¼ˆç®€ç§° Uï¼‰æ˜¯ä¸€ç»„ **ç»Ÿä¸€çš„ã€ä¸è§†å›¾æ— å…³çš„æŸ¥è¯¢ï¼ˆtokensï¼‰**ï¼Œç”¨äºåœ¨ transformer ä¸­èšåˆã€ç¼–ç å¹¶æŒæœ‰ _åœºæ™¯çº§åˆ«_ çš„ä¿¡æ¯ã€‚å®ƒä»¬å¯ä»¥æ˜¯ï¼š

*   Learnable slotsï¼ˆåƒ DETR çš„ object queriesï¼‰
*   å‚æ•°åŒ– 3D ç½‘æ ¼ / ä½“ç´ ä¸­å¿ƒ / å…³é”®ç‚¹é›†åˆ
*   Ray-queryï¼ˆæ¯ä¸ª query å¯¹åº”ä¸€ä¸ªå°„çº¿æˆ–åƒç´ ï¼‰

U çš„ä½œç”¨æ˜¯æŠŠè·¨è§†å›¾çš„è¯æ®é›†ä¸­åˆ°ä¸€ç»„æœ‰è¯­ä¹‰/å‡ ä½•æ„ä¹‰çš„ latent ä¸Šï¼Œä¾¿äºï¼š

*   æ±‡æ€»ä¿¡æ¯ï¼ˆRâ†’Uï¼‰ï¼Œ
*   å†³ç­–ï¼ˆä¾‹å¦‚å“ªäº›ç‚¹å¯ä¿¡ã€æ¯ä¸ª U å¯¹åº”å“ªä¸ª 3D åŸå­ï¼‰ï¼Œ
*   å†æŠŠç»“è®ºå†™å›åƒç´ çº§ï¼ˆUâ†’Rï¼‰ã€‚

### 4.2 å½¢å¼åŒ–

*   ä»¤ `U âˆˆ â„^{MÃ—D}`ï¼ˆM queriesï¼Œæ¯ä¸ª D ç»´ï¼‰ã€‚
*   åœ¨ Râ†’U ä¸­ç”¨ `U` ä½œä¸º `Q`ï¼ˆqueriesï¼‰ï¼Œç”¨ `R` ä½œä¸º `K/V`ã€‚
*   åœ¨ Uâ†’R ä¸­ç”¨ `R` ä½œä¸º `Q`ï¼Œç”¨ `U` ä½œä¸º `K/V`ã€‚

ï¼ˆè¯¦è§ Râ†’U / Uâ†’R ä¸‹é¢ï¼‰

### 4.3 Unified queries çš„å‡ ç§è¯­ä¹‰è®¾è®¡

*   **Point queries**ï¼šæ¯ä¸ª U å¯¹åº”ä¸€ä¸ª 3D point hypothesisï¼ˆå¸¸ç”¨äºå¯è§£é‡Šçš„ pointmapï¼‰ã€‚
*   **Ray queries**ï¼šæ¯ä¸ª U å¯¹åº”ä¸€æ¡åƒç´ å°„çº¿ï¼Œä»»åŠ¡æ˜¯æ²¿ ray å›å½’ depth / radianceã€‚
*   **Slot queries**ï¼šæ›´å¤šè¯­ä¹‰åŒ–ï¼Œåƒå¯¹è±¡ slotsï¼ˆç”¨äº instance level reconstructionï¼‰ã€‚

### 4.4 ä¼˜ç‚¹

*   æŠŠé«˜ç»´ NÃ—token é—®é¢˜å‹ç¼©ä¸º M queriesï¼ˆM â‰ª total tokensï¼‰ï¼Œèƒ½åšç¨€ç–æ±‡èšã€èŠ‚çœè®¡ç®—ã€‚
*   æ˜“äºå¼•å…¥å…ˆéªŒï¼ˆä¾‹å¦‚æŠŠ queries æ”¾åœ¨ 3D grid åˆå§‹ç½‘æ ¼ä¸Šï¼‰ï¼Œä¾¿äºè§£é‡Šä¸çº¦æŸã€‚

### 4.5 å®ç°è¦ç‚¹

*   U å¯ä»¥æ˜¯**learnable parameters**ï¼ˆéšæœºåˆå§‹åŒ–å¯å­¦ä¹ ï¼‰æˆ–**data-derived**ï¼ˆä¾‹å¦‚ä» coarse depth é‡‡æ ·çš„ 3D pointsï¼‰ã€‚
*   è®­ç»ƒæ—¶å¯¹ query åš slot-wise lossï¼ˆè‹¥çŸ¥é“ GT points å¯¹åº”å…³ç³»ï¼‰ï¼Œæˆ–ç”¨ reconstruction loss æŠŠ U è¯±å¯¼åˆ°æ­£ç¡®ä¿¡æ¯ä¸Šã€‚
*   è‹¥ U æ˜ å°„ä¸ºä¸‰ç»´ä½ç½®å»ºè®®ä¿ç•™ positional embeddingï¼ˆPEuï¼‰ã€‚

* * *

5 Râ†’U ä¸ Uâ†’Rï¼ˆä¸¤ç§ attention æ–¹å‘ï¼‰â€”â€” æ ¸å¿ƒäº¤äº’èŒƒå¼
=====================================

è¿™éƒ¨åˆ†æ˜¯ä½ é—®é¢˜ä¸­çš„å…³é”®ï¼šRï¼ˆview-pixel tokensï¼‰ä¸ Uï¼ˆunified queriesï¼‰ä¹‹é—´çš„åŒå‘äº¤äº’ï¼Œè§£é‡Šå…¶æ•°å­¦ã€ä¼ªç ã€ç›´è§‰ä¸ç”¨é€”ã€‚

### 5.1 Râ†’Uï¼ˆä»è§†å›¾å†™å…¥ç»Ÿä¸€æŸ¥è¯¢ï¼‰

**ç›´è§‰**ï¼šæŠŠæ‰€æœ‰è§†å›¾çš„è¯æ® **èšåˆï¼ˆæ±‡æ€»ï¼‰åˆ° unified queries**ã€‚æ¯ä¸ª U é€šè¿‡ attention ä»å¤šè§†å›¾ R ä¸­â€œæ”¶é›†â€ä¸è‡ªå·±ç›¸å…³çš„è¯æ®ï¼Œå½¢æˆåœºæ™¯çº§åˆ«çš„ latentã€‚

**æ•°å­¦ï¼ˆsingle attention blockï¼‰**ï¼š

*   è¾“å…¥ï¼š `U âˆˆ â„^{MÃ—D}`, `R âˆˆ â„^{NÃ—D}`ï¼ˆN = sum of tokens of all viewsï¼‰
*   Compute:
    *   Q = U W\_Q âˆˆ â„^{MÃ—d\_k}
    *   K = R W\_K âˆˆ â„^{NÃ—d\_k}
    *   V = R W\_V âˆˆ â„^{NÃ—d\_v}
    *   Attention: `A = softmax(Q K^T / sqrt(d_k))` âˆˆ â„^{MÃ—N}
    *   Output: `U' = A V` âˆˆ â„^{MÃ—d\_v}, æ¥ residual å’Œ FFN

**å«ä¹‰**ï¼šæ¯ä¸ª U æŸ¥è¯¢å“ªäº› Rï¼ˆåƒç´  tokensï¼‰æ˜¯ä¸å®ƒç›¸å…³çš„ï¼ŒæŠŠè¿™äº› R çš„å€¼æ±‡æ€»åˆ° Uã€‚

**ä¼ªç **:

```python
# R: [N, D], U: [M, D]
Q = U @ Wq   # [M, d]
K = R @ Wk   # [N, d]
V = R @ Wv   # [N, dv]
A = softmax(Q @ K.T / sqrt(d))
U_new = A @ V  # [M, dv]
U = LayerNorm(U + Linear(U_new))
```

**å·¥ç¨‹æ³¨æ„**ï¼š

*   N å¯ä»¥å¾ˆå¤§ï¼ˆå¤šè§†å›¾/é«˜åˆ†è¾¨ç‡ï¼‰ï¼Œè¦åš windowing æˆ–å°† R å‹ç¼©ï¼ˆavg pool / strided conv / learnable poolingï¼‰ä»¥é™ä½å¤æ‚åº¦ã€‚
*   å¯å¼•å…¥ **epipolar bias**ï¼šå½“ U ä»£è¡¨æŸ 3D point hypothesisï¼Œä¼˜å…ˆæ³¨æ„åˆ° R ä¸Šå…¶å¯¹åº” epipolar line çš„ tokensã€‚ï¼ˆé™ä½æœç´¢ç©ºé—´ï¼‰

* * *

### 5.2 Uâ†’Rï¼ˆä» unified queries å›å†™åˆ°åƒç´ ï¼‰

**ç›´è§‰**ï¼šæŠŠ U çš„å…¨å±€åœºæ™¯çŸ¥è¯†ï¼ˆä¾‹å¦‚æŸäº›ç‚¹çš„ä¸‰ç»´ä½ç½®ã€è¯­ä¹‰ï¼‰å†™å›ç»™åƒç´  tokensï¼Œä½¿åƒç´ å…·æœ‰æ›´å¼ºçš„å…¨å±€ä¸€è‡´æ€§ï¼ˆä¾¿äºæœ€ç»ˆçš„ per-pixel å›å½’ä»»åŠ¡ï¼‰ã€‚

**æ•°å­¦**ï¼ˆmirror of Râ†’Uï¼‰ï¼š

*   Q = R W\_Q âˆˆ â„^{NÃ—d}
*   K = U W\_K âˆˆ â„^{MÃ—d}
*   V = U W\_V âˆˆ â„^{MÃ—d\_v}
*   A = softmax(Q K^T / sqrt(d)) âˆˆ â„^{NÃ—M}
*   Output: `R' = A V` âˆˆ â„^{NÃ—d\_v}

**ä¼ªç **:

```python
# R: [N, D], U: [M, D]
Q = R @ Wq  # [N, d]
K = U @ Wk  # [M, d]
V = U @ Wv  # [M, dv]
A = softmax(Q @ K.T / sqrt(d))  # [N, M]
R_new = A @ V  # [N, dv]
R = LayerNorm(R + Linear(R_new))
```

**ä½œç”¨**ï¼š

*   åœ¨åƒç´ å±‚é¢è·å¾—è·¨è§†å›¾ä¸€è‡´çš„ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚â€œè¿™ä¸ªåƒç´ å±äº object slot #3 ä¸”å…¶å¯¹åº”3Dä½ç½®ä¸º Xâ€ï¼‰ï¼Œä»è€Œä½¿ depth/pointmap å›å½’æ›´ç²¾ç¡®ã€‚
*   Uâ†’R ä¹Ÿå¯è®¤ä¸ºæ˜¯ä¸€ä¸ª **conditional decoding**ï¼šU ç»™ R æä¾› contextã€‚

* * *

### 5.3 äº¤æ›¿ï¼ˆiterativeï¼‰Râ†’U / Uâ†’R çš„æ„ä¹‰

åœ¨å®è·µä¸­æŠŠ Râ†’U ä¸ Uâ†’R äº¤æ›¿å †å  B å±‚ï¼ˆæˆ–åŒæ—¶åœ¨æ¯å±‚å†…åšä¸¤æ¬¡ attentionï¼‰å¯ä»¥ï¼š

*   é€æ­¥æŠŠå±€éƒ¨è¯æ®æ±‡èšæˆå…¨å±€ç†è§£ï¼ˆRâ†’Uï¼‰ï¼Œ
*   å†æŠŠè¿™ä¸ªå…¨å±€ç†è§£ä¼ æ’­å›å„åƒç´ ä»¥æ”¹è¿›å±€éƒ¨é¢„æµ‹ï¼ˆUâ†’Rï¼‰ï¼Œ
*   å½¢æˆä¸€ä¸ª **â€œæ”¶é›†â€”èåˆâ€”å†™å›â€** çš„é—­ç¯ï¼Œé€å±‚ç²¾åŒ–ã€‚

è¿™æ˜¯ DUSt3R é‡Œ decoder ä¸¤åˆ†æ”¯äº¤äº’çš„æ ¸å¿ƒæ¨¡å¼ï¼ˆè®ºæ–‡ä¸­æœ‰äº¤æ›¿ attention çš„æè¿°ï¼‰ã€‚

* * *

### 5.4 shapes & complexity å®è·µå»ºè®®

*   è‹¥æ¯å›¾ `H'Ã—W' = 64Ã—64`ï¼ŒNâ‰ˆ4096ï¼ŒM å– 256 æˆ– 512 æ˜¯å¯è¡Œçš„ã€‚
*   è®¡ç®—å¤æ‚åº¦ï¼šRâ†’U cost ~ O(MÂ·NÂ·d). è‹¥ N å¾ˆå¤§ï¼ˆé«˜åˆ†è¾¨ç‡ or many viewsï¼‰è¦å‹ç¼©ï¼š
    *   ç”¨ **strided pooling** æŠŠ R å‹ç¼©ä¸ºè¾ƒå° token é›†åˆï¼Œæˆ–
    *   é‡‡ç”¨ **sparsity / routing**ï¼šåªè®©æ¯ U æ³¨æ„åˆ°éƒ¨åˆ† Rï¼ˆä¾‹å¦‚ä¾æ® epipolar constraint æˆ– learned routing moduleï¼‰ã€‚

* * *

6 occlusionã€maskã€confidence ä¸ä¸ç¡®å®šæ€§å¤„ç†
===================================

*   åœ¨ Râ†’U é˜¶æ®µï¼Œå¾ˆå¤š R tokens æ— æ•ˆï¼ˆå¤©ç©º/é•œé¢/é€æ˜ï¼‰ï¼Œå› æ­¤ **å¿…é¡»è¾“å‡º per-token confidence**ï¼ˆæˆ–åœ¨ attention é‡ŒåŠ å…¥ maskï¼‰ã€‚
*   è®­ç»ƒæ—¶å¯ä»¥ç”¨ **confidence-weighted loss**ï¼Œä¾‹å¦‚ paper ä¸­çš„ `Lconf = Î£ c_i * â„“_i âˆ’ Î± log c_i`ï¼ˆé¼“åŠ±åˆ†é…åˆç†ç½®ä¿¡åº¦ï¼Œé˜²æ­¢å…¨éƒ¨ câ†’0ï¼‰ã€‚
*   åœ¨ Uâ†’R å†™å›æ—¶ï¼Œè‹¥æŸ U çš„ä¿¡æ¯æ˜¯æ¥è‡ªè¢«é®æŒ¡çš„è§†å›¾ï¼Œconfidence ä¼šé™ä½å¯¹åº”å†™å›æƒé‡ï¼Œä»è€ŒæŠ‘åˆ¶é”™è¯¯ä¼ æ’­ã€‚

* * *

7 æŸå¤±ä¸è®­ç»ƒç»†èŠ‚ï¼ˆæŠŠ Râ†’U/Uâ†’R èå…¥è®­ç»ƒï¼‰
=========================

### 7.1 å¸¸è§ supervision

*   **Per-pixel 3D regression**ï¼š`L_point = Î£ ||P_pred(u,v) âˆ’ P_gt(u,v)||`ï¼ˆè‹¥æœ‰GTç‚¹ï¼‰
*   **Pairwise alignment**ï¼š`L_align = || R_transformed(P_A) âˆ’ P_B ||`
*   **Pose loss**ï¼šå¯¹ä½å§¿ head çš„ç½—å¾·é‡Œæ ¼/quat å›å½’æŸå¤±
*   **Photometric loss**ï¼ˆè‡ªç›‘ç£ï¼‰ï¼šç”¨é¢„æµ‹ depth + predicted pose åšé‡æŠ•å½±é‡å»ºå›¾ç‰‡å·®
*   **Slot supervisionï¼ˆå¯é€‰ï¼‰**ï¼šè‹¥ U è¢«è®¾è®¡ä¸ºç‰¹å®š 3D points çš„å®¹å™¨ï¼Œç»™ U ç›‘ç£ï¼ˆä¾‹å¦‚ object centersï¼‰

### 7.2 åœ¨ Râ†’U/Uâ†’R ä¸Šçš„ç‰¹æ®Šæ­£åˆ™

*   **Attention regularization**ï¼šé¼“åŠ± attention ç¨€ç–ï¼ˆL1 penalty on attention logitsï¼‰ä»¥é¿å…è¿‡åˆ†æ‰©æ•£è¯æ®ã€‚
*   **Geometric bias**ï¼šåœ¨ attention logits åŠ ä¸Š epipolar bias termï¼ˆæŠŠä¸ query å‡ ä½•ç›¸å…³ token çš„ logits å¢å¼ºï¼‰ã€‚
*   **Temperature scheduling**ï¼šè®­ç»ƒåˆæœŸ temperature å¤§ï¼ˆsoft attentionï¼‰ï¼ŒåæœŸé™ä½ï¼ˆæ›´ç¡¬çš„ assignmentï¼‰ã€‚

* * *

8 ä¼ªä»£ç ç¤ºä¾‹ï¼ˆæ•´ä½“ä¸€ä¸ª transformer layer åŒ…å« Râ†”U äº¤äº’ï¼‰
=========================================

```python
def RU_layer(R, U):
    # R: [N, D]  pixel tokens from all views
    # U: [M, D]  unified queries

    # R->U aggregation
    Q_U = U @ Wq_u      # [M, d]
    K_R = R @ Wk_r      # [N, d]
    V_R = R @ Wv_r      # [N, dv]
    A = softmax(Q_U @ K_R.T / sqrt(d))  # [M, N]
    U_new = A @ V_R     # [M, dv]
    U = LayerNorm(U + Linear_proj(U_new))

    # U->R write-back
    Q_R = R @ Wq_r2     # [N, d]
    K_U = U @ Wk_u2     # [M, d]
    V_U = U @ Wv_u2     # [M, dv]
    B = softmax(Q_R @ K_U.T / sqrt(d))  # [N, M]
    R_new = B @ V_U     # [N, dv]
    R = LayerNorm(R + Linear_proj(R_new))

    return R, U
```

* * *

9 å…¸å‹å˜ä½“ä¸è¿›é˜¶ç­–ç•¥
===========

*   **Epipolar-aware attention**ï¼šåœ¨ attention logits ä¸ŠåŠ å¯å­¦ä¹ åç½®  $b_{ij}$ ï¼ˆæˆ– maskï¼‰ä½¿å¾—ä¸ epipolar line ä¸åŒ¹é…çš„ pairs çš„ logits è¢«æŠ‘åˆ¶ã€‚
*   **Query initialization from coarse geometry**ï¼šæŠŠ U åˆå§‹ä¸ºä»ç²— depth map / SfM ç‚¹äº‘é‡‡æ ·å¾—åˆ°çš„ 3D hypothesesï¼Œè€Œä¸æ˜¯éšæœº learnableï¼Œè¿™æ ·è®­ç»ƒæ›´å¿«ã€è§£é‡Šæ€§æ›´å¼ºã€‚
*   **Slot merging / dynamic M**ï¼šåŠ¨æ€è°ƒæ•´ Mï¼ˆqueries æ•°é‡ï¼‰ï¼Œé€šè¿‡ clustering åˆå¹¶ç±»ä¼¼çš„ queriesã€‚
*   **Sparse attention & routing**ï¼šåˆ©ç”¨ routing ç½‘ç»œæŠŠæ¯ U åªè·¯ç”±åˆ°è‹¥å¹² Rï¼Œå‡å°è®¡ç®—å¹¶æé«˜èšç„¦æ€§ã€‚
*   **Multi-scale R/U**ï¼šåœ¨å¤šä¸ªå°ºåº¦ä¸Šç»´æŠ¤ R/Uï¼Œä½å°ºåº¦åšå…¨å±€èšåˆï¼Œé«˜å°ºåº¦åšç»†èŠ‚å›å†™ã€‚

* * *

10 å¸¸è§å·¥ç¨‹é—®é¢˜ä¸è°ƒä¼˜å»ºè®®
==============

1.  **å†…å­˜/è®¡ç®—ç“¶é¢ˆ**ï¼šNï¼ˆtokensï¼‰å’Œ Mï¼ˆqueriesï¼‰éƒ½ä¸å®œå¤ªå¤§ï¼›ä½¿ç”¨ windowed attentionã€strided poolingã€æˆ–åˆ†å—è®¡ç®—ã€‚
2.  **è®­ç»ƒä¸æ”¶æ•› / æ³¨æ„åŠ›â€œæ•£â€**ï¼šå°è¯• attention æ¸©åº¦ annealã€epipolar biasã€æˆ–åˆå§‹åŒ– U æ¥å¼•å¯¼æ”¶æ•›ã€‚
3.  **é®æŒ¡é”™è¯¯ä¼ æ’­**ï¼šå¿…é¡»æœ‰ç½®ä¿¡åº¦æœºåˆ¶æˆ–é®æŒ¡é¢„æµ‹ headã€‚
4.  **å°ºåº¦æ¼‚ç§»ï¼ˆsingle view/unknown intrinsicsï¼‰**ï¼šæ·»åŠ  scale head æˆ–ç”¨å¤–éƒ¨ depth priorï¼ˆDPTï¼‰æ¥æ ¡æ­£ã€‚
5.  **è°ƒå‚é¡ºåº**ï¼šå…ˆåœ¨åˆæˆå°é›†ä¸ŠéªŒè¯ loss design ä¸æ•°æ® pipelineï¼Œå† scale åˆ°æ··åˆçœŸå®æ•°æ®ã€‚

* * *

11 æŠŠæ¦‚å¿µå¸¦å› DUSt3R / VGGT / MapAnything çš„å…·ä½“å¯¹åº”
==========================================

*   **DUSt3R**ï¼šä½¿ç”¨ pixel-aligned pointmapsï¼ˆpixel features â†’ direct XYZï¼‰ï¼Œå†…éƒ¨é€šè¿‡ decoder çš„ cross-attention åš Râ†”R ä¸ Râ†”U é£æ ¼çš„èåˆï¼›global alignment åœ¨åç«¯æŠŠ pairwise pointmaps æ•´åˆæˆåœºæ™¯ã€‚DUSt3R çš„ decoder å°±æ˜¯åå¤åš Râ†”otherRï¼ˆäº¤æ›¿ attentionï¼‰ çš„ä¸€ç§å½¢å¼ã€‚
*   **VGGT**ï¼šæ˜ç¡®é‡‡ç”¨ frame-wise + global alternating attentionï¼ˆå³ Râ†’U / Uâ†’R é£æ ¼ï¼‰ä»¥æ‰©å±•åˆ°å¾ˆå¤šè§†å›¾ï¼›å¹¶æŠŠ unified queries è®¾è®¡æˆå¤šä»»åŠ¡ slotsï¼ˆposeã€depthã€tracksï¼‰ã€‚
*   **MapAnything**ï¼šåœ¨ U é‡ŒåŠ å…¥ metric tokens / external geometry tokensï¼ˆæŠŠå¤–éƒ¨ä¿¡æ¯ä½œä¸º U çš„ä¸€éƒ¨åˆ†ï¼‰ï¼Œå¹¶æŠŠ pixel-aligned featuresä¸ U åš Râ†’U/Uâ†’R äº¤äº’ä»¥äº§å‡º factored metric outputsã€‚

* * *

12 å°ç»“ï¼ˆå…³é”®è®°å¿†ç‚¹ï¼‰
============

*   **Pixel-aligned representation = ä¿ç•™ç©ºé—´ç»“æ„çš„ per-pixel feature + ä½ç½®ä¿¡æ¯ï¼ˆä¾¿äºåæŠ•å½±ï¼‰**ã€‚
*   **Unified queries = åœºæ™¯çº§åˆ«çš„æŸ¥è¯¢æ§½ï¼ˆM slotsï¼‰ï¼Œç”¨äºèšåˆ & å­˜å‚¨å…¨å±€ä¿¡æ¯**ã€‚
*   **Râ†’U = æŠŠå„è§†å›¾è¯æ®èšåˆåˆ° Uï¼ˆæ”¶é›†ï¼‰**ï¼›**Uâ†’R = æŠŠå…¨å±€çŸ¥è¯†å†™å› pixel tokensï¼ˆå¹¿æ’­/ç²¾åŒ–ï¼‰**ã€‚
*   ä¸¤è€…äº¤æ›¿æ˜¯ç°ä»£ transformer-based å¤šè§†å‡ ä½•æ¡†æ¶ï¼ˆDUSt3R / VGGT / MapAnythingï¼‰çš„æ ¸å¿ƒäº¤äº’èŒƒå¼ã€‚

* * *

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥**é©¬ä¸Šä¸¤ä»¶äº‹**ä¸­çš„ä»»ä¸€ä»¶ï¼ˆæˆ–éƒ½åšï¼‰ï¼š

*   A. æŠŠä¸Šé¢å†…å®¹ç”»æˆä¸€å¼ æ¸…æ™°çš„æµç¨‹å›¾ï¼ˆåŒ…å« Râ†’U / Uâ†’R ç®­å¤´ã€tensor shapesã€ç¤ºä¾‹ attention çŸ©é˜µï¼‰å¹¶ç”Ÿæˆ PNG/PDFï¼›
*   B. ç»™å‡ºå®Œæ•´ PyTorch é£æ ¼çš„å‚è€ƒå®ç°ï¼ˆ`encoder.py`ã€`ru_block.py`ã€`heads.py`ï¼‰ä¼ªä»£ç /å¯è¿è¡Œ skeletonï¼Œæ–¹ä¾¿ä½ ç›´æ¥åœ¨ä»£ç åº“é‡Œè¯•éªŒã€‚

ä½ é€‰ A æˆ– Bï¼ˆæˆ–ä¸¤è€…ï¼‰ï¼Œæˆ‘ç«‹åˆ»ç”Ÿæˆã€‚



---
Powered by [ChatGPT Exporter](https://www.chatgptexporter.com)
